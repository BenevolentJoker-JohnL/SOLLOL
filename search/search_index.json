{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SOLLOL Documentation","text":"<p>Hybrid Cluster Orchestrator: Task Routing + Distributed Model Inference</p> <p>Quick Start View on GitHub</p>"},{"location":"#what-is-sollol","title":"What is SOLLOL?","text":"<p>SOLLOL is a hybrid cluster orchestrator that unifies task routing and distributed model inference for local LLM deployments.</p> <p>SOLLOL provides two complementary distribution strategies:</p>"},{"location":"#1-task-distribution-load-balancing","title":"1. Task Distribution (Load Balancing)","text":"<p>Intelligently routes multiple agent requests across Ollama nodes based on: - Task complexity (embedding vs generation vs analysis) - Resource availability (GPU memory, CPU load) - Real-time performance (latency, success rate) - Historical patterns (adaptive learning from past executions)</p>"},{"location":"#2-model-sharding-layer-level-distribution","title":"2. Model Sharding (Layer-Level Distribution)","text":"<p>Enables running models that don't fit on a single machine via llama.cpp RPC: - Layer distribution across multiple RPC backends - GGUF auto-extraction from Ollama blob storage - Verified with 13B models across 2-3 nodes - Trade-offs: Slower startup (2-5 min) and inference (~5 tok/s vs ~20 tok/s local)</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Intelligent Routing</p> <p>Context-aware analysis routes requests to optimal nodes using multi-factor scoring and adaptive learning.</p> <p> Learn more</p> </li> <li> <p> Priority Queue</p> <p>10-level priority system with age-based fairness ensures critical tasks get resources without starvation.</p> <p> Learn more</p> </li> <li> <p> Auto Failover</p> <p>3 retry attempts with exponential backoff and health monitoring ensure zero-downtime operation.</p> <p> Learn more</p> </li> <li> <p> Observability</p> <p>Real-time dashboard with Prometheus metrics and routing transparency for complete visibility.</p> <p> Learn more</p> </li> <li> <p> High Performance</p> <p>Ray actors and Dask batch processing deliver &lt;10ms routing overhead with 40-60% throughput gains.</p> <p> Learn more</p> </li> <li> <p> Enterprise Security</p> <p>SHA-256 API keys with RBAC permissions and per-key rate limiting for production deployments.</p> <p> Learn more</p> </li> </ul>"},{"location":"#performance-impact","title":"Performance Impact","text":"Metric Expected Improvement Avg Latency -30-40% (context-aware routing to optimal nodes) P95 Latency -40-50% (avoiding overloaded nodes) Success Rate +2-4pp (automatic failover and retry) Throughput +40-60% (better resource utilization) GPU Utilization +50-80% (intelligent task-to-GPU matching) <p>Production Ready</p> <p>SOLLOL is fully functional and production-ready. No artificial limits, no feature gates.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Architecture Overview - Understand the system design</li> <li>Deployment Guide - Deploy with Docker or Kubernetes</li> <li>Benchmarks - See performance methodology and results</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions or share ideas</li> <li>Contributing: See contribution guidelines</li> <li>Enterprise: Explore enterprise features</li> </ul> <p>Built with  by BenevolentJoker-JohnL</p> <p>GitHub Sponsor</p>"},{"location":"GPU_DETECTION_SETUP/","title":"GPU Detection and Reporting Setup Guide","text":"<p>This guide walks through configuring SOLLOL for automatic GPU detection across your distributed cluster.</p>"},{"location":"GPU_DETECTION_SETUP/#overview","title":"Overview","text":"<p>SOLLOL's GPU detection system enables intelligent routing by detecting GPU capabilities on remote RPC nodes. The system uses Redis as a central registry where nodes publish their GPU specifications.</p>"},{"location":"GPU_DETECTION_SETUP/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Coordinator Node (10.9.66.154)                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  Redis (0.0.0.0:6379)                          \u2502    \u2502\n\u2502  \u2502  - Central GPU registry                        \u2502    \u2502\n\u2502  \u2502  - Stores node capabilities                    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502  SOLLOL Discovery                              \u2502    \u2502\n\u2502  \u2502  - Reads GPU info from Redis                   \u2502    \u2502\n\u2502  \u2502  - Routes requests to GPU nodes                \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u25b2\n                       \u2502 Publish GPU specs via Redis\n                       \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502             \u2502             \u2502             \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502GPU Node \u2502   \u2502GPU Node \u2502  \u2502CPU Node \u2502  \u2502GPU Node \u2502\n    \u2502.90:50052\u2502   \u2502.45:50052\u2502  \u2502.48:50052\u2502  \u2502.X:50052 \u2502\n    \u2502RTX 3090 \u2502   \u2502RTX 3080 \u2502  \u2502CPU-only \u2502  \u2502  ...    \u2502\n    \u250224GB VRAM\u2502   \u250210GB VRAM\u2502  \u250216GB RAM \u2502  \u2502         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502             \u2502             \u2502             \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                Run register_gpu_node.py on startup\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#prerequisites","title":"Prerequisites","text":"<ul> <li>SOLLOL installed on coordinator node</li> <li>Redis installed and running on coordinator</li> <li>Python 3.8+ on all nodes</li> <li>Network connectivity between nodes (ports 6379, 50052)</li> <li>NVIDIA drivers on GPU nodes (for CUDA detection)</li> </ul>"},{"location":"GPU_DETECTION_SETUP/#part-1-configure-redis-for-network-access","title":"Part 1: Configure Redis for Network Access","text":"<p>By default, Redis only listens on <code>localhost</code> (127.0.0.1). Remote GPU nodes need network access to register their capabilities.</p>"},{"location":"GPU_DETECTION_SETUP/#step-11-edit-redis-configuration","title":"Step 1.1: Edit Redis Configuration","text":"<p>On the coordinator node (10.9.66.154):</p> <pre><code># Open Redis config\nsudo nano /etc/redis/redis.conf\n\n# Find the line:\nbind 127.0.0.1 ::1\n\n# Change it to (replace with your coordinator IP):\nbind 127.0.0.1 ::1 10.9.66.154\n</code></pre> <p>What this does: Allows Redis to accept connections from the network while still listening on localhost.</p>"},{"location":"GPU_DETECTION_SETUP/#step-12-restart-redis","title":"Step 1.2: Restart Redis","text":"<pre><code>sudo systemctl restart redis\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#step-13-verify-network-listening","title":"Step 1.3: Verify Network Listening","text":"<pre><code># Check Redis is listening on network interface\nnetstat -tuln | grep 6379\n\n# Expected output:\n# tcp  0  0 127.0.0.1:6379      0.0.0.0:*  LISTEN  (localhost)\n# tcp  0  0 10.9.66.154:6379    0.0.0.0:*  LISTEN  (network)\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#step-14-test-remote-connection","title":"Step 1.4: Test Remote Connection","text":"<p>From a remote node (e.g., 10.9.66.90):</p> <pre><code># Test Redis connectivity\nredis-cli -h 10.9.66.154 ping\n\n# Expected output:\n# PONG\n</code></pre> <p>If you get \"Connection refused\", check: - Firewall rules (see Security section below) - Redis bind configuration - Network connectivity (<code>ping 10.9.66.154</code>)</p>"},{"location":"GPU_DETECTION_SETUP/#part-2-register-gpu-nodes","title":"Part 2: Register GPU Nodes","text":"<p>Each GPU node needs to publish its capabilities to Redis on startup.</p>"},{"location":"GPU_DETECTION_SETUP/#step-21-copy-registration-script-to-gpu-nodes","title":"Step 2.1: Copy Registration Script to GPU Nodes","text":"<p>From the coordinator:</p> <pre><code># Copy registration script to each GPU node\nscp /home/joker/SOLLOL/scripts/register_gpu_node.py 10.9.66.90:~/\nscp /home/joker/SOLLOL/scripts/register_gpu_node.py 10.9.66.45:~/\n# ... repeat for all GPU nodes\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#step-22-install-dependencies-on-gpu-nodes","title":"Step 2.2: Install Dependencies on GPU Nodes","text":"<p>On each GPU node:</p> <pre><code># Install Python Redis client\npip install redis\n\n# Verify nvidia-smi is available (for GPU detection)\nnvidia-smi\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#step-23-run-registration-script","title":"Step 2.3: Run Registration Script","text":"<p>On each GPU node (e.g., 10.9.66.90):</p> <pre><code># Register GPU with coordinator\npython3 register_gpu_node.py --redis-host 10.9.66.154\n\n# Expected output:\n# ======================================================================\n# GPU NODE REGISTRATION - SOLLOL\n# ======================================================================\n#\n# \ud83d\udccd Node IP: 10.9.66.90\n#\n# \ud83d\udd0d Detecting resources...\n#\n# ======================================================================\n# DETECTED RESOURCES\n# ======================================================================\n# \u2705 GPU(s) Found: 1\n#    GPU 0: NVIDIA GeForce RTX 3090 (cuda:0) - 19200 MB VRAM\n#\n# \ud83d\udcbe CPU RAM: 12000 MB\n# \u26a1 Parallel Workers: 2\n#\n# ======================================================================\n# RPC-SERVER COMMAND\n# ======================================================================\n# rpc-server --host 0.0.0.0 --port 50052 --device cpu,cuda:0 --mem 12000,19200\n#\n# ======================================================================\n# REDIS REGISTRATION\n# ======================================================================\n# \u2705 Published to Redis: redis://10.9.66.154:6379\n#    Key: sollol:rpc:node:10.9.66.90:50052\n#    TTL: 1 hour\n#\n# ======================================================================\n# \u2705 REGISTRATION COMPLETE\n# ======================================================================\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#step-24-start-rpc-server-with-detected-configuration","title":"Step 2.4: Start RPC Server with Detected Configuration","text":"<p>Use the command shown by the registration script:</p> <pre><code># Example from script output:\nnohup rpc-server --host 0.0.0.0 --port 50052 --device cpu,cuda:0 --mem 12000,19200 &gt; /tmp/rpc-server.log 2&gt;&amp;1 &amp;\n</code></pre> <p>What this does: - Starts RPC server with hybrid CPU + GPU workers - CPU worker: 12GB RAM - GPU worker: 19.2GB VRAM (80% of total, 20% reserved) - 2 parallel workers on this node</p>"},{"location":"GPU_DETECTION_SETUP/#part-3-verify-gpu-detection-on-coordinator","title":"Part 3: Verify GPU Detection on Coordinator","text":"<p>On the coordinator node (10.9.66.154):</p>"},{"location":"GPU_DETECTION_SETUP/#step-31-check-redis-registration","title":"Step 3.1: Check Redis Registration","text":"<pre><code># List all registered nodes\nredis-cli KEYS \"sollol:rpc:node:*\"\n\n# Expected output:\n# 1) \"sollol:rpc:node:10.9.66.90:50052\"\n# 2) \"sollol:rpc:node:10.9.66.45:50052\"\n# 3) \"sollol:rpc:node:10.9.66.48:50052\"\n\n# View specific node info\nredis-cli GET \"sollol:rpc:node:10.9.66.90:50052\"\n\n# Expected output (JSON):\n# {\"has_gpu\":true,\"gpu_devices\":[\"cuda:0\"],\"gpu_vram_mb\":[19200],\"gpu_names\":[\"NVIDIA GeForce RTX 3090\"],\"cpu_ram_mb\":12000,\"device_config\":\"cpu,cuda:0\",\"memory_config\":\"12000,19200\",\"total_parallel_workers\":2}\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#step-32-test-sollol-discovery","title":"Step 3.2: Test SOLLOL Discovery","text":"<pre><code>cd /home/joker/SOLLOL\n\nPYTHONPATH=src python3 -c \"\nfrom sollol.rpc_discovery import auto_discover_rpc_backends, detect_node_resources\nimport json\n\nprint('=== RPC Node Discovery ===')\nbackends = auto_discover_rpc_backends()\nprint(f'Found {len(backends)} RPC backends:')\n\nfor backend in backends:\n    host = backend['host']\n    port = backend.get('port', 50052)\n    print(f'\\n\ud83d\udccd {host}:{port}')\n\n    resources = detect_node_resources(host)\n    print(f'   Has GPU: {resources[\\\"has_gpu\\\"]}')\n    print(f'   GPU devices: {resources.get(\\\"gpu_devices\\\", [])}')\n    print(f'   GPU VRAM: {resources.get(\\\"gpu_vram_mb\\\", [])} MB')\n    print(f'   CPU RAM: {resources.get(\\\"cpu_ram_mb\\\", 0)} MB')\n    print(f'   Workers: {resources[\\\"total_parallel_workers\\\"]}')\n\"\n</code></pre> <p>Expected output:</p> <pre><code>=== RPC Node Discovery ===\nFound 3 RPC backends:\n\n\ud83d\udccd 10.9.66.90:50052\n   Has GPU: True\n   GPU devices: ['cuda:0']\n   GPU VRAM: [19200] MB\n   CPU RAM: 12000 MB\n   Workers: 2\n\n\ud83d\udccd 10.9.66.45:50052\n   Has GPU: True\n   GPU devices: ['cuda:0']\n   GPU VRAM: [10240] MB\n   CPU RAM: 8000 MB\n   Workers: 2\n\n\ud83d\udccd 10.9.66.48:50052\n   Has GPU: False\n   GPU devices: []\n   GPU VRAM: [] MB\n   CPU RAM: 16000 MB\n   Workers: 1\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#part-4-automate-registration-on-startup","title":"Part 4: Automate Registration on Startup","text":"<p>GPU registrations expire after 1 hour. Set up automatic re-registration.</p>"},{"location":"GPU_DETECTION_SETUP/#option-a-systemd-service-recommended","title":"Option A: Systemd Service (Recommended)","text":"<p>Create <code>/etc/systemd/system/sollol-gpu-reporter.service</code> on each GPU node:</p> <pre><code>[Unit]\nDescription=SOLLOL GPU Registration Service\nAfter=network.target redis.service\n\n[Service]\nType=simple\nUser=your-username\nWorkingDirectory=/home/your-username\nExecStart=/usr/bin/python3 /home/your-username/register_gpu_node.py --redis-host 10.9.66.154\nRestart=always\nRestartSec=3600\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable sollol-gpu-reporter\nsudo systemctl start sollol-gpu-reporter\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#option-b-cron-job","title":"Option B: Cron Job","text":"<p>Add to crontab on each GPU node:</p> <pre><code>crontab -e\n\n# Add this line (runs every hour):\n0 * * * * cd /home/your-username &amp;&amp; python3 register_gpu_node.py --redis-host 10.9.66.154 &gt; /tmp/gpu-registration.log 2&gt;&amp;1\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#security-considerations","title":"Security Considerations","text":""},{"location":"GPU_DETECTION_SETUP/#firewall-configuration","title":"Firewall Configuration","text":"<p>On the coordinator node:</p> <pre><code># Allow Redis from trusted subnet only\nsudo ufw allow from 10.9.66.0/24 to any port 6379 comment \"Redis from cluster nodes\"\n\n# Reload firewall\nsudo ufw reload\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#redis-authentication-optional-but-recommended","title":"Redis Authentication (Optional but Recommended)","text":"<p>Add password protection to Redis:</p> <pre><code># On coordinator, edit Redis config\nsudo nano /etc/redis/redis.conf\n\n# Add this line:\nrequirepass your_strong_password_here\n\n# Restart Redis\nsudo systemctl restart redis\n</code></pre> <p>Update registration script usage:</p> <pre><code># On GPU nodes, set password\nexport REDIS_PASSWORD=\"your_strong_password_here\"\n\n# Or pass via URL\npython3 register_gpu_node.py --redis-host \"redis://:your_strong_password_here@10.9.66.154:6379\"\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"GPU_DETECTION_SETUP/#issue-redis-connection-refused-from-remote-nodes","title":"Issue: Redis connection refused from remote nodes","text":"<p>Symptoms: <pre><code>\u274c Failed to publish to Redis: Error 111 connecting to 10.9.66.154:6379. Connection refused.\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check Redis is listening on network:    <pre><code>netstat -tuln | grep 6379\n# Should show: 10.9.66.154:6379\n</code></pre></p> </li> <li> <p>Verify bind configuration:    <pre><code>redis-cli CONFIG GET bind\n# Should include your coordinator IP\n</code></pre></p> </li> <li> <p>Check firewall:    <pre><code>sudo ufw status\n# Should allow port 6379 from cluster subnet\n</code></pre></p> </li> <li> <p>Test connectivity:    <pre><code># From remote node\ntelnet 10.9.66.154 6379\n# Should connect (press Ctrl+] then 'quit' to exit)\n</code></pre></p> </li> </ol>"},{"location":"GPU_DETECTION_SETUP/#issue-gpu-not-detected-on-node","title":"Issue: GPU not detected on node","text":"<p>Symptoms: <pre><code>\u2139\ufe0f  No GPU detected (CPU-only node)\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check NVIDIA drivers:    <pre><code>nvidia-smi\n# Should show GPU info\n</code></pre></p> </li> <li> <p>Verify nvidia-smi in PATH:    <pre><code>which nvidia-smi\n# Should return: /usr/bin/nvidia-smi (or similar)\n</code></pre></p> </li> <li> <p>Run registration with verbose output:    <pre><code>python3 -u register_gpu_node.py --redis-host 10.9.66.154\n</code></pre></p> </li> </ol>"},{"location":"GPU_DETECTION_SETUP/#issue-registration-expires-too-quickly","title":"Issue: Registration expires too quickly","text":"<p>Symptoms: GPU info disappears after 1 hour</p> <p>Solutions:</p> <ol> <li>Set up systemd service (see Part 4, Option A)</li> <li> <p>Automatically re-registers every hour</p> </li> <li> <p>Increase TTL (modify <code>register_gpu_node.py</code>):    <pre><code># Line 158: Change ex=3600 to ex=86400 (24 hours)\nr.set(key, json.dumps(resources), ex=86400)\n</code></pre></p> </li> </ol>"},{"location":"GPU_DETECTION_SETUP/#issue-coordinator-shows-has-gpu-false-after-registration","title":"Issue: Coordinator shows \"Has GPU: False\" after registration","text":"<p>Symptoms: <pre><code>redis-cli GET \"sollol:rpc:node:10.9.66.90:50052\"\n# Returns valid JSON with has_gpu:true\n\n# But SOLLOL discovery shows:\n# Has GPU: False\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check Redis connection in SOLLOL:    <pre><code># Verify SOLLOL can reach Redis\nredis-cli -h localhost ping\n</code></pre></p> </li> <li> <p>Verify key format:    <pre><code># Keys must match pattern: sollol:rpc:node:&lt;ip&gt;:&lt;port&gt;\nredis-cli KEYS \"sollol:rpc:node:*\"\n</code></pre></p> </li> <li> <p>Check SOLLOL discovery code:    <pre><code># In src/sollol/rpc_discovery.py\n# Ensure SOLLOL_REDIS_URL env var is set correctly\nexport SOLLOL_REDIS_URL=\"redis://localhost:6379\"\n</code></pre></p> </li> </ol>"},{"location":"GPU_DETECTION_SETUP/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"GPU_DETECTION_SETUP/#multi-gpu-nodes","title":"Multi-GPU Nodes","text":"<p>For nodes with multiple GPUs:</p> <pre><code># register_gpu_node.py automatically detects all GPUs\npython3 register_gpu_node.py --redis-host 10.9.66.154\n\n# Example output for 2-GPU node:\n# \u2705 GPU(s) Found: 2\n#    GPU 0: NVIDIA RTX 3090 (cuda:0) - 19200 MB VRAM\n#    GPU 1: NVIDIA RTX 3080 (cuda:1) - 10240 MB VRAM\n#\n# \u26a1 Parallel Workers: 3 (1 CPU + 2 GPU)\n#\n# RPC-SERVER COMMAND:\n# rpc-server --host 0.0.0.0 --port 50052 --device cpu,cuda:0,cuda:1 --mem 12000,19200,10240\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#custom-memory-allocation","title":"Custom Memory Allocation","text":"<p>Override automatic VRAM detection:</p> <pre><code># Modify register_gpu_node.py before running:\n# Line 102: Change 0.8 (80%) to your desired percentage\nsafe_vram = int(total_vram * 0.7)  # Use 70% instead of 80%\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#amdintel-gpu-support","title":"AMD/Intel GPU Support","text":"<p>For non-NVIDIA GPUs, you'll need to modify the detection logic:</p> <pre><code># In register_gpu_node.py, add AMD GPU detection\ndef get_amd_gpus():\n    \"\"\"Detect AMD GPUs using rocm-smi\"\"\"\n    try:\n        result = subprocess.run(\n            [\"rocm-smi\", \"--showproductname\"],\n            capture_output=True,\n            text=True,\n            check=True\n        )\n        # Parse rocm-smi output...\n        return gpus\n    except:\n        return []\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#architecture-notes","title":"Architecture Notes","text":""},{"location":"GPU_DETECTION_SETUP/#why-redis","title":"Why Redis?","text":"<ol> <li>Centralized discovery: Single source of truth for cluster capabilities</li> <li>Automatic expiration: Stale nodes auto-removed (TTL-based)</li> <li>Fast lookups: O(1) key-value retrieval</li> <li>Network-accessible: Supports distributed clusters</li> <li>Atomic updates: Race-condition free registration</li> </ol>"},{"location":"GPU_DETECTION_SETUP/#why-not-direct-gpu-queries","title":"Why Not Direct GPU Queries?","text":"<p>Direct SSH or RPC-based GPU queries have drawbacks:</p> <ul> <li>\u274c Slower (network round-trip per query)</li> <li>\u274c Requires credentials (SSH keys, auth)</li> <li>\u274c Tight coupling (coordinator needs node access)</li> <li>\u274c No caching (repeated queries for same info)</li> </ul> <p>Redis registration is:</p> <ul> <li>\u2705 Fast (local Redis lookup)</li> <li>\u2705 Decoupled (nodes self-register)</li> <li>\u2705 Cached (1-hour TTL)</li> <li>\u2705 Scalable (add nodes without config changes)</li> </ul>"},{"location":"GPU_DETECTION_SETUP/#related-documentation","title":"Related Documentation","text":"<ul> <li>CUDA RPC Build Guide - Building CUDA-enabled binaries</li> <li>Bare Metal Deployment - Production setup with systemd</li> <li>Hybrid Parallelization - CPU + GPU worker configuration</li> </ul>"},{"location":"GPU_DETECTION_SETUP/#quick-reference","title":"Quick Reference","text":""},{"location":"GPU_DETECTION_SETUP/#essential-commands","title":"Essential Commands","text":"<pre><code># Configure Redis for network access\nsudo nano /etc/redis/redis.conf  # Add coordinator IP to bind line\nsudo systemctl restart redis\n\n# Register GPU node\npython3 register_gpu_node.py --redis-host 10.9.66.154\n\n# Verify registration\nredis-cli KEYS \"sollol:rpc:node:*\"\nredis-cli GET \"sollol:rpc:node:10.9.66.90:50052\"\n\n# Test SOLLOL discovery\nPYTHONPATH=src python3 -c \"from sollol.rpc_discovery import auto_discover_rpc_backends; print(auto_discover_rpc_backends())\"\n</code></pre>"},{"location":"GPU_DETECTION_SETUP/#file-locations","title":"File Locations","text":"<ul> <li>Registration script: <code>/home/joker/SOLLOL/scripts/register_gpu_node.py</code></li> <li>Redis config: <code>/etc/redis/redis.conf</code></li> <li>Systemd service: <code>/etc/systemd/system/sollol-gpu-reporter.service</code></li> <li>Discovery code: <code>/home/joker/SOLLOL/src/sollol/rpc_discovery.py</code></li> </ul>"},{"location":"GPU_DETECTION_SETUP/#support","title":"Support","text":"<p>For issues or questions: - GitHub Issues: https://github.com/BenevolentJoker-JohnL/SOLLOL/issues - Documentation: https://github.com/BenevolentJoker-JohnL/SOLLOL/tree/main/docs</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/","title":"Hybrid GPU+CPU RPC Parallelization","text":""},{"location":"HYBRID_RPC_PARALLELIZATION/#the-problem","title":"The Problem","text":"<p>Traditional RPC setups treat each node as a single worker: - CPU nodes: 1 worker (RAM only) - GPU nodes: 1 worker (VRAM only)</p> <p>This wastes resources! A GPU node has BOTH VRAM (for GPU) AND RAM (for CPU).</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#the-solution-hybrid-parallelization","title":"The Solution: Hybrid Parallelization","text":"<p>Configure GPU nodes to contribute multiple workers: - 1 CPU worker (using RAM) - 1+ GPU workers (using VRAM)</p> <p>All devices work in parallel on the same physical machine!</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#example-3-physical-nodes-4-parallel-workers","title":"Example: 3 Physical Nodes \u2192 4 Parallel Workers","text":""},{"location":"HYBRID_RPC_PARALLELIZATION/#traditional-setup-3-workers","title":"Traditional Setup (3 workers):","text":"<pre><code>CPU Node 1  \u2192 1 worker (8GB RAM)\nCPU Node 2  \u2192 1 worker (8GB RAM)\nGPU Node    \u2192 1 worker (12GB VRAM)\nTotal: 3 parallel workers\n</code></pre>"},{"location":"HYBRID_RPC_PARALLELIZATION/#hybrid-setup-4-workers","title":"Hybrid Setup (4 workers):","text":"<pre><code>CPU Node 1  \u2192 1 worker (8GB RAM)\nCPU Node 2  \u2192 1 worker (8GB RAM)\nGPU Node    \u2192 2 workers:\n              \u251c\u2500 CPU device (10GB RAM)\n              \u2514\u2500 GPU device (9.6GB VRAM)\nTotal: 4 parallel workers (+33% throughput!)\n</code></pre>"},{"location":"HYBRID_RPC_PARALLELIZATION/#how-it-works","title":"How It Works","text":"<p>llama.cpp's <code>rpc-server</code> supports multiple devices via <code>--device</code>:</p> <pre><code># CPU-only node\nrpc-server --host 0.0.0.0 --port 50052 --device cpu --mem 8000\n\n# GPU node with HYBRID parallelization\nrpc-server --host 0.0.0.0 --port 50052 --device cpu,cuda:0 --mem 10000,9600\n                                                  ^^^  ^^^^^^       ^^^^^  ^^^^\n                                                  CPU   GPU         CPU    GPU\n                                                        device      RAM    VRAM\n</code></pre> <p>When the coordinator distributes model layers: 1. Some layers go to CPU workers (uses RAM) 2. Other layers go to GPU workers (uses VRAM) 3. All workers process in parallel</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#layer-distribution-example-40-layers","title":"Layer Distribution Example (40 layers)","text":"<p>With 2 CPU nodes + 1 hybrid GPU node:</p> <pre><code>CPU Node 1:      Layers 0-9   (10 layers, 8GB RAM)\nCPU Node 2:      Layers 10-19 (10 layers, 8GB RAM)\nGPU Node CPU:    Layers 20-29 (10 layers, 10GB RAM)\nGPU Node GPU:    Layers 30-39 (10 layers, 9.6GB VRAM) \u26a1\n</code></pre> <p>All 4 workers compute simultaneously!</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#automatic-configuration","title":"Automatic Configuration","text":"<p>SOLLOL automatically configures hybrid GPU+CPU parallelization!</p> <p>When you start SOLLOL with auto-discovery, it: 1. Detects all local resources (GPUs, CPUs, RAM, VRAM) 2. Calculates safe allocations (80% with 20% reserve) 3. Starts RPC servers with optimal hybrid device configs</p> <p>No manual configuration needed!</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#manual-configuration-optional","title":"Manual Configuration (Optional)","text":"<p>If you want to see what SOLLOL would configure, run the detection script:</p> <pre><code>python scripts/setup_rpc_node.py\n</code></pre> <p>Output: <pre><code>======================================================================\nRPC NODE SETUP - Hybrid GPU+CPU Parallelization\n======================================================================\n\n\ud83d\udd0d Detecting local resources...\n\n======================================================================\nDETECTED RESOURCES\n======================================================================\n\u2705 GPU(s) Found: 1\n   GPU 0: cuda:0 - 9600 MB VRAM (safe allocation)\n\n\ud83d\udcbe CPU RAM: 10240 MB (safe allocation)\n\n\u26a1 Total Parallel Workers: 2\n   (1 CPU worker + 1 GPU worker(s))\n\n======================================================================\nGENERATED RPC-SERVER COMMAND\n======================================================================\nrpc-server --host 0.0.0.0 --port 50052 --device cpu,cuda:0 --mem 10240,9600\n\n\ud83d\udca1 This command creates HYBRID parallelization:\n   \u2022 CPU device processes layers using 10240 MB RAM\n   \u2022 cuda:0 processes layers using 9600 MB VRAM\n\n   ALL 2 devices work IN PARALLEL on this single node!\n</code></pre></p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#safety-features","title":"Safety Features","text":"<ul> <li>Auto 20% reserve: Leaves headroom to prevent OOM crashes</li> <li>Per-device limits: Each worker has its own safe memory allocation</li> <li>Vendor detection: Supports NVIDIA (cuda), AMD (rocm), Intel</li> <li>Fallback: Gracefully falls back to CPU-only if no GPU detected</li> </ul>"},{"location":"HYBRID_RPC_PARALLELIZATION/#benefits","title":"Benefits","text":"<ol> <li>More throughput: Extra parallel workers without extra hardware</li> <li>Better utilization: Use ALL resources (RAM + VRAM)</li> <li>No coordinator bottleneck: Computation is distributed</li> <li>Safe allocations: 80% limits prevent crashes</li> <li>Auto-detection: No manual config needed</li> </ol>"},{"location":"HYBRID_RPC_PARALLELIZATION/#comparison","title":"Comparison","text":"Metric Traditional Hybrid Improvement Physical nodes 3 3 Same Parallel workers 3 4 +33% GPU utilization 100% 100% Same CPU utilization 100% 100% Same RAM waste High None Maximized <p>With hybrid parallelization, you get more workers using the same hardware!</p>"},{"location":"HYBRID_RPC_PARALLELIZATION/#future-multi-gpu-nodes","title":"Future: Multi-GPU Nodes","text":"<p>For nodes with 2+ GPUs:</p> <pre><code># 4 workers on 1 machine!\nrpc-server --device cpu,cuda:0,cuda:1,cuda:2 --mem 10000,9600,9600,9600\n</code></pre> <p>Physical setup: - 2 CPU nodes: 2 workers - 1 quad-GPU node: 4 workers (1 CPU + 3 GPUs)</p> <p>Total: 6 parallel workers across 3 physical machines!</p>"},{"location":"MULTI_APP_OBSERVABILITY/","title":"Multi-App Observability with SOLLOL","text":""},{"location":"MULTI_APP_OBSERVABILITY/#overview","title":"Overview","text":"<p>SOLLOL v0.9.16+ supports multi-app observability, allowing multiple applications on the same machine to share a single unified dashboard instance. This provides centralized monitoring for all SOLLOL-powered applications without port conflicts or resource duplication.</p>"},{"location":"MULTI_APP_OBSERVABILITY/#how-it-works","title":"How It Works","text":""},{"location":"MULTI_APP_OBSERVABILITY/#automatic-fallback-mechanism","title":"Automatic Fallback Mechanism","text":"<p>When a SOLLOL application attempts to start the Unified Dashboard:</p> <ol> <li>Port Check: The dashboard checks if port 8080 (default) is already in use</li> <li>Fallback Detection: If occupied, assumes another SOLLOL dashboard is running</li> <li>Graceful Fallback: Logs connection info and continues without error</li> <li>Shared Observability: Both applications use the same dashboard for monitoring</li> </ol>"},{"location":"MULTI_APP_OBSERVABILITY/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Zero Configuration: Automatic detection and fallback</li> <li>\u2705 No Port Conflicts: Multiple apps coexist peacefully</li> <li>\u2705 Centralized Monitoring: Single dashboard for all applications</li> <li>\u2705 Graceful Degradation: Apps continue running if dashboard unavailable</li> </ul>"},{"location":"MULTI_APP_OBSERVABILITY/#usage","title":"Usage","text":""},{"location":"MULTI_APP_OBSERVABILITY/#basic-example","title":"Basic Example","text":"<pre><code>from sollol import OllamaPool, UnifiedDashboard, RayHybridRouter\n\n# Create application infrastructure\npool = OllamaPool(nodes=[{\"host\": \"localhost\", \"port\": 11434}])\nrouter = RayHybridRouter(ollama_pool=pool, enable_distributed=True)\n\n# Create dashboard with fallback enabled (default)\ndashboard = UnifiedDashboard(router=router, dashboard_port=8080)\n\n# Start dashboard - automatically falls back if port occupied\ndashboard.run(allow_fallback=True)  # allow_fallback=True is default\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#expected-behavior","title":"Expected Behavior","text":"<p>First Application (starts dashboard): <pre><code>2025-10-07 09:00:00,000 - INFO - \ud83d\ude80 Starting Unified Dashboard on http://0.0.0.0:8080\n2025-10-07 09:00:00,100 - INFO - \u2705 Using Waitress production server\n</code></pre></p> <p>Second Application (detects existing dashboard): <pre><code>2025-10-07 09:01:00,000 - INFO - \ud83d\udcca Dashboard already running on port 8080\n2025-10-07 09:01:00,001 - INFO -    Connecting to existing dashboard at http://localhost:8080\n2025-10-07 09:01:00,002 - INFO - \u2705 Application will use shared dashboard for observability\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#configuration-options","title":"Configuration Options","text":""},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-initialization","title":"Dashboard Initialization","text":"<pre><code>dashboard = UnifiedDashboard(\n    router=router,\n    dashboard_port=8080,        # Dashboard port (default: 8080)\n    ray_dashboard_port=8265,    # Ray dashboard port (default: 8265)\n    dask_dashboard_port=8787,   # Dask dashboard port (default: 8787)\n)\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#run-method","title":"Run Method","text":"<pre><code>dashboard.run(\n    host=\"0.0.0.0\",            # Bind address (default: 0.0.0.0)\n    debug=False,               # Debug mode (default: False)\n    allow_fallback=True        # Enable fallback detection (default: True)\n)\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#disable-fallback-force-start","title":"Disable Fallback (Force Start)","text":"<p>If you want to force the dashboard to start and fail if the port is occupied:</p> <pre><code>dashboard.run(allow_fallback=False)\n</code></pre> <p>This will raise an <code>OSError</code> if the port is already in use.</p>"},{"location":"MULTI_APP_OBSERVABILITY/#architecture","title":"Architecture","text":""},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-components","title":"Dashboard Components","text":"<p>The Unified Dashboard provides monitoring for:</p> <ol> <li>Network Nodes: Ollama pool nodes with health status</li> <li>RPC Backends: llama.cpp RPC servers for model sharding</li> <li>Applications: Registered applications using SOLLOL</li> <li>Request Metrics: Real-time request/response stats</li> <li>Ray Dashboard: Distributed task monitoring (port 8265)</li> <li>Dask Dashboard: Batch processing monitoring (dynamic port)</li> </ol>"},{"location":"MULTI_APP_OBSERVABILITY/#multi-app-workflow","title":"Multi-App Workflow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application 1  \u2502         \u2502  Application 2  \u2502\n\u2502  (SynapticLlamas\u2502         \u2502  (CustomApp)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                           \u2502\n         \u2502 1. Start dashboard        \u2502 2. Detect existing\n         \u2502    on port 8080           \u2502    dashboard on 8080\n         \u2502                           \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u25bc               \u25bc           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Unified Dashboard (Port 8080)    \u2502\n    \u2502                                    \u2502\n    \u2502  \u2022 Network Nodes                   \u2502\n    \u2502  \u2022 RPC Backends                    \u2502\n    \u2502  \u2022 Applications: 2 registered      \u2502\n    \u2502  \u2022 Request Metrics                 \u2502\n    \u2502  \u2022 Ray Dashboard \u2192 :8265           \u2502\n    \u2502  \u2022 Dask Dashboard \u2192 :auto          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#real-world-example","title":"Real-World Example","text":""},{"location":"MULTI_APP_OBSERVABILITY/#synapticllamas-integration","title":"SynapticLlamas Integration","text":"<pre><code># In SynapticLlamas main.py\nfrom sollol import UnifiedDashboard, RayHybridRouter\nfrom sollol.dashboard_client import DashboardClient\n\n# Create distributed router\nrouter = RayHybridRouter(\n    ollama_pool=ollama_pool,\n    rpc_backends=rpc_backends,\n    enable_distributed=True\n)\n\n# Register application with dashboard\nclient = DashboardClient(\n    app_name=\"SynapticLlamas\",\n    router_type=\"RayHybridRouter\",\n    version=\"1.0.0\",\n    dashboard_url=\"http://localhost:8080\",\n    auto_register=True\n)\n\n# In dashboard command handler\nif command == 'dashboard':\n    dashboard = UnifiedDashboard(\n        router=router,\n        dashboard_port=8080\n    )\n    dashboard.run(allow_fallback=True)  # Graceful fallback\n</code></pre> <p>If another SOLLOL app (like a custom inference service) is already running with a dashboard, SynapticLlamas will detect it and share the same dashboard.</p>"},{"location":"MULTI_APP_OBSERVABILITY/#testing","title":"Testing","text":""},{"location":"MULTI_APP_OBSERVABILITY/#test-script","title":"Test Script","text":"<pre><code># Start first app (SynapticLlamas)\ncd ~/SynapticLlamas\npython3 main.py --distributed\n# Type: dashboard\n\n# In another terminal, test fallback\ncd ~/SOLLOL\npython3 test_dashboard_fallback_simple.py\n</code></pre> <p>Expected output: <pre><code>\u2705 Dashboard is already running on port 8080\nTesting fallback behavior:\nAttempting to start dashboard on port 8080 (already occupied)...\n\ud83d\udcca Dashboard already running on port 8080\n   Connecting to existing dashboard at http://localhost:8080\n\u2705 Application will use shared dashboard for observability\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-not-accessible","title":"Dashboard Not Accessible","text":"<p>Issue: Dashboard fallback detected but cannot access http://localhost:8080</p> <p>Solution: Check if the first application's dashboard is actually running: <pre><code>curl http://localhost:8080/api/health\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#port-conflicts","title":"Port Conflicts","text":"<p>Issue: Want to run dashboards on different ports for different apps</p> <p>Solution: Use custom ports for each application: <pre><code># App 1\ndashboard1 = UnifiedDashboard(router=router1, dashboard_port=8080)\n\n# App 2\ndashboard2 = UnifiedDashboard(router=router2, dashboard_port=8081)\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#disable-fallback","title":"Disable Fallback","text":"<p>Issue: Need to ensure dashboard starts fresh each time</p> <p>Solution: Disable fallback and handle errors manually: <pre><code>try:\n    dashboard.run(allow_fallback=False)\nexcept OSError as e:\n    if \"Address already in use\" in str(e):\n        # Kill existing process and retry\n        subprocess.run([\"pkill\", \"-f\", \"waitress\"])\n        dashboard.run(allow_fallback=False)\n    else:\n        raise\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#best-practices","title":"Best Practices","text":"<ol> <li>Enable Fallback by Default: Use <code>allow_fallback=True</code> for production apps</li> <li>Single Dashboard Per Machine: Let one \"primary\" app run the dashboard</li> <li>Dashboard Client Registration: Register all apps with <code>DashboardClient</code> for visibility</li> <li>Health Checks: Monitor <code>/api/health</code> endpoint for dashboard availability</li> <li>Graceful Shutdown: Ensure dashboard cleanup on app exit</li> </ol>"},{"location":"MULTI_APP_OBSERVABILITY/#api-reference","title":"API Reference","text":""},{"location":"MULTI_APP_OBSERVABILITY/#unifieddashboardrun","title":"UnifiedDashboard.run()","text":"<pre><code>def run(self, host: str = \"0.0.0.0\", debug: bool = False, allow_fallback: bool = True) -&gt; None:\n    \"\"\"\n    Run dashboard server (production-ready with Waitress).\n\n    Args:\n        host: Bind address (default: 0.0.0.0)\n        debug: Enable Flask debug mode (default: False)\n        allow_fallback: If True and port is in use, assume another dashboard is running (default: True)\n\n    Raises:\n        OSError: If port is in use and allow_fallback=False\n    \"\"\"\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-endpoints","title":"Dashboard Endpoints","text":"<ul> <li><code>GET /</code> - Dashboard UI</li> <li><code>GET /api/health</code> - Health check</li> <li><code>GET /api/metrics</code> - Current metrics</li> <li><code>GET /api/dashboard/config</code> - Dashboard configuration (Ray/Dask ports)</li> <li><code>GET /api/applications</code> - Registered applications</li> <li><code>GET /api/nodes</code> - Ollama pool nodes</li> <li><code>GET /api/rpc_backends</code> - RPC backend status</li> <li><code>WS /events</code> - Real-time event stream</li> </ul>"},{"location":"MULTI_APP_OBSERVABILITY/#version-history","title":"Version History","text":"<ul> <li>v0.9.16: Added multi-app fallback with <code>allow_fallback</code> parameter</li> <li>v0.9.15: Added SOLLOL version logging</li> <li>v0.9.14: Ray OOM prevention</li> <li>v0.9.13: Fixed async/dict errors in metrics endpoint</li> <li>v0.9.12: Optimized panel sizing</li> <li>v0.9.7: Added dynamic port detection for Dask</li> </ul>"},{"location":"MULTI_APP_OBSERVABILITY/#see-also","title":"See Also","text":"<ul> <li>Unified Dashboard Documentation</li> <li>Ray Integration Guide</li> <li>Dask Integration Guide</li> <li>Application Registration</li> </ul>"},{"location":"layer_partitioning/","title":"Layer Partitioning for Large Models","text":"<p>SOLLOL now supports layer partitioning - the ability to split large models (70B+) across multiple nodes for distributed inference.</p>"},{"location":"layer_partitioning/#overview","title":"Overview","text":"<p>What it does: - Splits models too large for a single GPU across multiple nodes - Each node loads specific layers (e.g., node1: layers 0-39, node2: layers 40-79) - Coordinates inference across nodes automatically - Provides both vertical scaling (bigger models) and horizontal scaling (more throughput)</p> <p>When to use it: - Models larger than your single-node GPU memory (Llama-70B, Mixtral-8x7B, etc.) - You have multiple GPUs across different machines - You want to run models that wouldn't fit otherwise</p>"},{"location":"layer_partitioning/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    SOLLOL Gateway                    \u2502\n\u2502                                                      \u2502\n\u2502  Request for llama2:70b                              \u2502\n\u2502    \u2193                                                 \u2502\n\u2502  Routing Decision:                                   \u2502\n\u2502    - Small model (llama3.2) \u2192 Individual node        \u2502\n\u2502    - Large model (llama2:70b) \u2192 Cluster              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                             \u2502\n        \u25bc                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Individual     \u2502         \u2502   Node Cluster   \u2502\n\u2502      Node        \u2502         \u2502   (llama2:70b)   \u2502\n\u2502                  \u2502         \u2502                  \u2502\n\u2502  \u2022 llama3.2      \u2502         \u2502  Node 1:         \u2502\n\u2502  \u2022 phi           \u2502         \u2502    Layers 0-39   \u2502\n\u2502  \u2022 codellama     \u2502         \u2502                  \u2502\n\u2502                  \u2502         \u2502  Node 2:         \u2502\n\u2502  Full model      \u2502         \u2502    Layers 40-79  \u2502\n\u2502  on single GPU   \u2502         \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  Distributed     \u2502\n                             \u2502  inference       \u2502\n                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"layer_partitioning/#quick-start","title":"Quick Start","text":""},{"location":"layer_partitioning/#1-add-nodes-to-registry","title":"1. Add Nodes to Registry","text":"<pre><code>from sollol.registry import NodeRegistry\n\nregistry = NodeRegistry()\n\n# Add individual nodes\nregistry.add_node(\"http://192.168.1.10:11434\", name=\"gpu-node-1\")\nregistry.add_node(\"http://192.168.1.11:11434\", name=\"gpu-node-2\")\nregistry.add_node(\"http://192.168.1.12:11434\", name=\"gpu-node-3\")\n</code></pre>"},{"location":"layer_partitioning/#2-create-cluster-for-large-model","title":"2. Create Cluster for Large Model","text":"<pre><code># Create cluster for Llama-70B across 2 nodes\ncluster = registry.create_cluster(\n    name=\"llama70b-cluster\",\n    node_urls=[\n        \"http://192.168.1.10:11434\",\n        \"http://192.168.1.11:11434\"\n    ],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"even\"  # or \"memory_aware\"\n)\n\n# Output:\n# \ud83d\udce6 Created cluster 'llama70b-cluster' with 2 nodes for llama2:70b\n#    Node 1: layers 0-39 (40 layers)\n#    Node 2: layers 40-79 (40 layers)\n</code></pre>"},{"location":"layer_partitioning/#3-use-smart-routing","title":"3. Use Smart Routing","text":"<pre><code># Get best worker for model (automatically selects cluster for large models)\nworker = registry.get_worker_for_model(\"llama2:70b\")\n\nif isinstance(worker, NodeCluster):\n    print(f\"Using cluster: {worker.name}\")\n    result = await worker.generate(\"Explain quantum computing\")\nelse:\n    print(f\"Using single node: {worker.name}\")\n</code></pre>"},{"location":"layer_partitioning/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"layer_partitioning/#even-distribution-default","title":"Even Distribution (Default)","text":"<p>Splits layers evenly across nodes:</p> <pre><code>cluster = registry.create_cluster(\n    name=\"my-cluster\",\n    node_urls=[...],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"even\"\n)\n\n# 80 layers across 2 nodes:\n# Node 1: 40 layers (0-39)\n# Node 2: 40 layers (40-79)\n</code></pre>"},{"location":"layer_partitioning/#memory-aware-distribution","title":"Memory-Aware Distribution","text":"<p>Allocates layers proportionally to available memory:</p> <pre><code>cluster = registry.create_cluster(\n    name=\"my-cluster\",\n    node_urls=[...],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"memory_aware\"\n)\n\n# If Node 1 has 48GB and Node 2 has 24GB:\n# Node 1: 53 layers (0-52)  # 66% of layers\n# Node 2: 27 layers (53-79)  # 33% of layers\n</code></pre>"},{"location":"layer_partitioning/#supported-models","title":"Supported Models","text":""},{"location":"layer_partitioning/#large-models-require-partitioning","title":"Large Models (Require Partitioning)","text":"<ul> <li>llama2:70b - 80 layers, ~36GB memory</li> <li>llama3:70b - 80 layers, ~36GB memory</li> <li>mixtral:8x7b - 32 layers (MoE), ~26GB memory</li> </ul>"},{"location":"layer_partitioning/#small-models-single-node","title":"Small Models (Single Node)","text":"<ul> <li>llama3.2 - 32 layers, ~2GB memory</li> <li>phi - 32 layers, ~1.5GB memory</li> <li>codellama:7b - 32 layers, ~4GB memory</li> </ul>"},{"location":"layer_partitioning/#adding-custom-models","title":"Adding Custom Models","text":"<p>Edit <code>sollol/node_cluster.py</code> to add model specs:</p> <pre><code>MODEL_SPECS = {\n    \"your-model:70b\": ModelSpec(\n        name=\"your-model:70b\",\n        total_layers=80,\n        memory_per_layer_mb=450,\n        min_memory_mb=4096\n    ),\n}\n</code></pre>"},{"location":"layer_partitioning/#health-checking","title":"Health Checking","text":"<p>Clusters require ALL nodes to be healthy:</p> <pre><code># Check cluster health\nis_healthy = await cluster.health_check()\n\nif not is_healthy:\n    print(f\"Cluster unhealthy - nodes down: {[n.url for n in cluster.nodes if not n.is_healthy]}\")\n\n# Check all clusters\ncluster_health = await registry.health_check_clusters()\n</code></pre>"},{"location":"layer_partitioning/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom sollol.registry import NodeRegistry\n\nasync def main():\n    # Setup registry\n    registry = NodeRegistry()\n\n    # Discover nodes on network\n    registry.discover_nodes(cidr=\"192.168.1.0/24\")\n\n    # Create cluster for large model\n    if len(registry.get_healthy_nodes()) &gt;= 2:\n        cluster = registry.create_cluster(\n            name=\"llama70b\",\n            node_urls=[\n                registry.get_healthy_nodes()[0].url,\n                registry.get_healthy_nodes()[1].url\n            ],\n            model=\"llama2:70b\"\n        )\n\n        # Run inference\n        result = await cluster.generate(\n            prompt=\"Write a detailed explanation of quantum entanglement\",\n            options={\"temperature\": 0.7}\n        )\n\n        print(result['response'])\n        print(f\"\\nCluster info: {result['_cluster']}\")\n\n    # Small models use individual nodes\n    worker = registry.get_worker_for_model(\"llama3.2\")\n    print(f\"Small model routed to: {worker.name}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"layer_partitioning/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"layer_partitioning/#latency-trade-offs","title":"Latency Trade-offs","text":"<ul> <li>Single Node: Fastest (no inter-node communication)</li> <li>Cluster (2 nodes): ~10-20% slower due to coordination overhead</li> <li>Cluster (3+ nodes): Additional latency per node</li> </ul>"},{"location":"layer_partitioning/#throughput-benefits","title":"Throughput Benefits","text":"<ul> <li>Load Balancing: Small models spread across available nodes</li> <li>Large Model Access: Run models impossible on single GPU</li> <li>Mixed Workloads: Clusters handle 70B while other nodes serve 7B/13B</li> </ul>"},{"location":"layer_partitioning/#optimal-configurations","title":"Optimal Configurations","text":"<p>2-Node Cluster (Recommended for 70B models) <pre><code>GPU 1: 24GB - Layers 0-39 of Llama-70B\nGPU 2: 24GB - Layers 40-79 of Llama-70B\n</code></pre></p> <p>3-Node Cluster (For extremely large or multiple 70B models) <pre><code>GPU 1: 24GB - Layers 0-26\nGPU 2: 24GB - Layers 27-53\nGPU 3: 24GB - Layers 54-79\n</code></pre></p>"},{"location":"layer_partitioning/#limitations","title":"Limitations","text":"<p>Current implementation: - \u2705 Layer partitioning logic and cluster management - \u2705 Health checking and failover - \u2705 Smart routing (large \u2192 cluster, small \u2192 single) - \u26a0\ufe0f  Inter-node communication (basic implementation) - \u26a0\ufe0f  Requires Ollama layer partitioning support (WIP upstream)</p> <p>Future enhancements: 1. gRPC for faster inter-node communication 2. Session affinity for multi-turn conversations 3. Dynamic layer rebalancing based on load 4. Automatic cluster creation on demand</p>"},{"location":"layer_partitioning/#comparison-sollol-vs-ollol","title":"Comparison: SOLLOL vs OLLOL","text":"Feature SOLLOL OLLOL (K2/olol) Load balancing \u2705 Advanced \u2705 Basic Layer partitioning \u2705 New \u2705 Existing Health scoring \u2705 Performance-based \u2705 Simple ping Auto-discovery \u2705 CIDR scanning \u2705 Broadcast Intelligent routing \u2705 Task-aware \u274c Priority queuing \u2705 \u274c Observability \u2705 Dashboard \u274c <p>SOLLOL now provides both capabilities from OLLOL: - Load balancing across independent workers (existing) - Layer partitioning for large models (new)</p>"},{"location":"layer_partitioning/#see-also","title":"See Also","text":"<ul> <li>Node Registry Documentation</li> <li>Intelligent Routing</li> <li>Network Discovery</li> </ul>"},{"location":"llama_cpp_guide/","title":"llama.cpp Model Sharding Guide","text":"<p>Complete guide to running large language models across multiple machines using SOLLOL's llama.cpp integration.</p>"},{"location":"llama_cpp_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>What is Model Sharding?</li> <li>Architecture</li> <li>When to Use Model Sharding</li> <li>Setup Guide</li> <li>Usage Examples</li> <li>Model Profiles</li> <li>Performance &amp; Optimization</li> <li>Troubleshooting</li> <li>Advanced Topics</li> </ol>"},{"location":"llama_cpp_guide/#overview","title":"Overview","text":"<p>SOLLOL integrates with llama.cpp to enable model sharding - the ability to run models that are too large to fit on a single GPU by distributing them across multiple machines.</p>"},{"location":"llama_cpp_guide/#key-benefits","title":"Key Benefits","text":"<ul> <li>\u2705 Run 70B+ models on machines with limited VRAM</li> <li>\u2705 Automatic GGUF extraction from Ollama storage</li> <li>\u2705 Zero-config setup with auto-discovery</li> <li>\u2705 Seamless integration with SOLLOL's intelligent routing</li> <li>\u2705 Hybrid operation - small models use Ollama, large models use sharding</li> </ul>"},{"location":"llama_cpp_guide/#what-you-get","title":"What You Get","text":"<pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\n# Auto-configure with model sharding enabled\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    num_rpc_backends=3  # Shard across 3 machines\n)\n\n# Small models \u2192 Ollama (fast, local)\nresponse = router.route_request(\n    model=\"llama3.2\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Large models \u2192 llama.cpp sharding (distributed)\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[{\"role\": \"user\", \"content\": \"Complex task...\"}]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#what-is-model-sharding","title":"What is Model Sharding?","text":""},{"location":"llama_cpp_guide/#the-problem","title":"The Problem","text":"<p>Large language models like Llama 3.1 70B require ~40GB of VRAM. If you only have GPUs with 24GB VRAM, you can't run these models locally.</p> <p>Traditional options: - \u274c Cloud APIs (expensive, privacy concerns) - \u274c Upgrade to more expensive hardware - \u274c Use smaller, less capable models</p>"},{"location":"llama_cpp_guide/#the-solution-model-sharding","title":"The Solution: Model Sharding","text":"<p>Model sharding distributes a single model across multiple machines:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Llama 3.1 70B Model (40GB total)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502              \u2502              \u2502\n        \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Machine 1  \u2502 \u2502   Machine 2  \u2502 \u2502   Machine 3  \u2502\n\u2502              \u2502 \u2502              \u2502 \u2502              \u2502\n\u2502 Layers 0-26  \u2502 \u2502 Layers 27-53 \u2502 \u2502 Layers 54-79 \u2502\n\u2502   (~13GB)    \u2502 \u2502   (~13GB)    \u2502 \u2502   (~13GB)    \u2502\n\u2502              \u2502 \u2502              \u2502 \u2502              \u2502\n\u2502 RTX 4090     \u2502 \u2502 RTX 4090     \u2502 \u2502 RTX 4090     \u2502\n\u2502  24GB VRAM   \u2502 \u2502  24GB VRAM   \u2502 \u2502  24GB VRAM   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>How it works: 1. Model layers are split across machines 2. During inference, data flows through each machine sequentially 3. llama.cpp RPC (Remote Procedure Call) handles communication 4. SOLLOL coordinates everything automatically</p>"},{"location":"llama_cpp_guide/#architecture","title":"Architecture","text":""},{"location":"llama_cpp_guide/#components","title":"Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      SOLLOL Gateway                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              HybridRouter                            \u2502  \u2502\n\u2502  \u2502  \u2022 Analyzes model requirements                       \u2502  \u2502\n\u2502  \u2502  \u2022 Routes small models \u2192 Ollama                      \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Routes large models \u2192 llama.cpp coordinator    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502                         \u2502\n     \u25bc                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Ollama    \u2502       \u2502   llama.cpp Coordinator              \u2502\n\u2502   Nodes     \u2502       \u2502   (llama-server)                     \u2502\n\u2502             \u2502       \u2502                                      \u2502\n\u2502 \u2022 llama3.2  \u2502       \u2502   \u2022 Loads GGUF model                 \u2502\n\u2502 \u2022 phi       \u2502       \u2502   \u2022 Distributes layers to RPC nodes  \u2502\n\u2502 \u2022 codellama \u2502       \u2502   \u2022 Coordinates inference            \u2502\n\u2502             \u2502       \u2502   \u2022 Returns results to SOLLOL        \u2502\n\u2502  (Fast,     \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502   local)    \u2502                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502                     \u2502              \u2502\n                        \u25bc                     \u25bc              \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 RPC Node \u2502          \u2502 RPC Node \u2502  \u2502 RPC Node \u2502\n                  \u2502    #1    \u2502          \u2502    #2    \u2502  \u2502    #3    \u2502\n                  \u2502          \u2502          \u2502          \u2502  \u2502          \u2502\n                  \u2502 Layers   \u2502          \u2502 Layers   \u2502  \u2502 Layers   \u2502\n                  \u2502  0-26    \u2502          \u2502  27-53   \u2502  \u2502  54-79   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"llama_cpp_guide/#key-components-explained","title":"Key Components Explained","text":"<p>1. HybridRouter - Analyzes incoming requests - Determines if model needs sharding - Routes to appropriate backend</p> <p>2. llama.cpp Coordinator (llama-server) - Central control process - Loads the GGUF model file - Distributes layers to RPC backends - Coordinates inference passes</p> <p>3. RPC Backends (rpc-server) - Worker processes on each machine - Execute inference for assigned layers - Communicate via gRPC</p> <p>4. GGUF Extraction - SOLLOL automatically finds GGUFs in Ollama storage - No manual file management needed</p>"},{"location":"llama_cpp_guide/#when-to-use-model-sharding","title":"When to Use Model Sharding","text":""},{"location":"llama_cpp_guide/#use-model-sharding-when","title":"Use Model Sharding When:","text":"<p>\u2705 Model is too large for single GPU - Llama 3.1 70B (~40GB) on 24GB GPUs - Mixtral 8x7B (~26GB) on 16GB GPUs - Any model &gt; available VRAM</p> <p>\u2705 You have multiple machines with GPUs - 2-4 machines with GPUs - Network connection between them - Want to utilize distributed resources</p> <p>\u2705 Throughput is acceptable - Understand ~2-5x slower than local inference - Startup time (2-5 minutes) is acceptable - Network latency is reasonable (&lt;10ms)</p>"},{"location":"llama_cpp_guide/#dont-use-model-sharding-when","title":"Don't Use Model Sharding When:","text":"<p>\u274c Model fits on single GPU - Use Ollama directly (much faster) - Example: Llama 3.2 3B, Phi-3, CodeLlama 7B</p> <p>\u274c Need lowest latency - Model sharding adds network overhead - Better: Use smaller model or upgrade hardware</p> <p>\u274c Poor network connectivity - High latency (&gt;50ms) kills performance - RPC requires fast, reliable network</p>"},{"location":"llama_cpp_guide/#setup-guide","title":"Setup Guide","text":""},{"location":"llama_cpp_guide/#prerequisites","title":"Prerequisites","text":"<p>Hardware: - 2+ machines with GPUs (or CPUs for testing) - Network connectivity between machines - Sufficient VRAM across machines for model</p> <p>Software: - Python 3.8+ - Ollama installed (for GGUF extraction) - CMake (for building llama.cpp) - Git</p>"},{"location":"llama_cpp_guide/#option-1-auto-setup-recommended","title":"Option 1: Auto-Setup (Recommended)","text":"<p>SOLLOL can automatically setup llama.cpp RPC backends:</p> <pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\n# Auto-setup everything\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    auto_discover_rpc=True,    # Try to find existing RPC servers\n    auto_setup_rpc=True,        # Build/start RPC if not found\n    num_rpc_backends=3          # Number of RPC servers to start\n)\n\n# SOLLOL will:\n# 1. Look for running RPC servers on the network\n# 2. If not found, clone llama.cpp repository\n# 3. Build llama.cpp with RPC support\n# 4. Start RPC servers on available ports\n# 5. Configure HybridRouter to use them\n</code></pre> <p>What auto-setup does: 1. Checks for <code>llama.cpp</code> directory in <code>~/llama.cpp</code> 2. If not found, clones from GitHub 3. Builds with <code>cmake -DGGML_RPC=ON</code> 4. Starts <code>rpc-server</code> processes on ports 50052, 50053, etc. 5. Configures coordinator to use these backends</p>"},{"location":"llama_cpp_guide/#option-2-manual-setup","title":"Option 2: Manual Setup","text":"<p>For more control, setup llama.cpp manually:</p> <p>Step 1: Install llama.cpp</p> <pre><code># Clone llama.cpp\ncd ~\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\n# Build with RPC support\ncmake -B build -DGGML_RPC=ON -DLLAMA_CURL=OFF\ncmake --build build --config Release -j$(nproc)\n</code></pre> <p>Step 2: Start RPC Servers</p> <p>On each machine that will participate in sharding:</p> <pre><code># Machine 1\n~/llama.cpp/build/bin/rpc-server -H 0.0.0.0 -p 50052\n\n# Machine 2\n~/llama.cpp/build/bin/rpc-server -H 0.0.0.0 -p 50052\n\n# Machine 3\n~/llama.cpp/build/bin/rpc-server -H 0.0.0.0 -p 50052\n</code></pre> <p>Step 3: Configure SOLLOL</p> <pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\n# Manual RPC backend configuration\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    rpc_backends=[\n        {\"host\": \"192.168.1.10\", \"port\": 50052},\n        {\"host\": \"192.168.1.11\", \"port\": 50052},\n        {\"host\": \"192.168.1.12\", \"port\": 50052},\n    ]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#option-3-using-environment-variables","title":"Option 3: Using Environment Variables","text":"<pre><code># Set RPC backends via environment\nexport RPC_BACKENDS=\"192.168.1.10:50052,192.168.1.11:50052,192.168.1.12:50052\"\n\n# Run SOLLOL gateway\npython -m sollol.gateway\n</code></pre> <pre><code># HybridRouter will pick up RPC_BACKENDS automatically\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True\n)\n</code></pre>"},{"location":"llama_cpp_guide/#verification","title":"Verification","text":"<p>Check that RPC backends are accessible:</p> <pre><code># Test RPC connectivity\nnc -zv 192.168.1.10 50052\nnc -zv 192.168.1.11 50052\nnc -zv 192.168.1.12 50052\n</code></pre> <pre><code># Verify in Python\nfrom sollol.rpc_discovery import test_rpc_backend\n\nresult = test_rpc_backend(\"192.168.1.10\", 50052)\nprint(f\"RPC backend: {'\u2713 Available' if result else '\u2717 Not available'}\")\n</code></pre>"},{"location":"llama_cpp_guide/#usage-examples","title":"Usage Examples","text":""},{"location":"llama_cpp_guide/#example-1-basic-model-sharding","title":"Example 1: Basic Model Sharding","text":"<pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\nfrom sollol.priority_helpers import Priority\n\n# Setup router with model sharding\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    num_rpc_backends=3\n)\n\n# Small model - uses Ollama (fast)\nprint(\"Running small model...\")\nresponse = router.route_request(\n    model=\"llama3.2\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    priority=Priority.HIGH\n)\nprint(f\"Backend: {response.get('_routing', {}).get('backend')}\")\n# Output: Backend: ollama-pool\n\n# Large model - uses llama.cpp sharding (distributed)\nprint(\"\\nRunning large model...\")\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    priority=Priority.NORMAL\n)\nprint(f\"Backend: {response.get('_routing', {}).get('backend')}\")\n# Output: Backend: llama.cpp-distributed\n</code></pre>"},{"location":"llama_cpp_guide/#example-2-check-model-routing-decision","title":"Example 2: Check Model Routing Decision","text":"<pre><code># Check which backend will be used before making request\nmodel = \"llama3.1:70b\"\nwill_use_sharding = router.should_use_distributed(model)\n\nif will_use_sharding:\n    print(f\"{model} will use distributed inference (llama.cpp)\")\n    print(\"Expected: Slower startup, network overhead\")\nelse:\n    print(f\"{model} will use local Ollama\")\n    print(\"Expected: Fast, low latency\")\n</code></pre>"},{"location":"llama_cpp_guide/#example-3-monitor-coordinator-status","title":"Example 3: Monitor Coordinator Status","text":"<pre><code># Get coordinator information\nif router.coordinator:\n    print(f\"Coordinator running: {router.coordinator.is_running()}\")\n    print(f\"Coordinator model: {router.coordinator_model}\")\n    print(f\"RPC backends: {len(router.coordinator.rpc_backends)}\")\n    print(f\"Coordinator URL: {router.coordinator.base_url}\")\nelse:\n    print(\"No coordinator active (using Ollama only)\")\n</code></pre>"},{"location":"llama_cpp_guide/#example-4-async-usage","title":"Example 4: Async Usage","text":"<pre><code>import asyncio\nfrom sollol import HybridRouter, OllamaPool\n\nasync def run_distributed_inference():\n    # Create router (async version)\n    pool = await OllamaPool.auto_configure()\n    router = HybridRouter(\n        ollama_pool=pool,\n        enable_distributed=True,\n        num_rpc_backends=3\n    )\n\n    # Run inference\n    response = await router.route_request(\n        model=\"llama3.1:70b\",\n        messages=[{\"role\": \"user\", \"content\": \"What is AGI?\"}]\n    )\n\n    print(response['message']['content'])\n\nasyncio.run(run_distributed_inference())\n</code></pre>"},{"location":"llama_cpp_guide/#example-5-multi-agent-with-mixed-models","title":"Example 5: Multi-Agent with Mixed Models","text":"<pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\nfrom sollol.priority_helpers import get_priority_for_role\n\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    num_rpc_backends=3\n)\n\nagents = [\n    {\"name\": \"Researcher\", \"role\": \"researcher\", \"model\": \"llama3.1:70b\"},  # Sharded\n    {\"name\": \"Editor\", \"role\": \"editor\", \"model\": \"llama3.2\"},              # Local\n    {\"name\": \"Summarizer\", \"role\": \"summarizer\", \"model\": \"llama3.2\"},      # Local\n]\n\nfor agent in agents:\n    priority = get_priority_for_role(agent[\"role\"])\n\n    response = router.route_request(\n        model=agent[\"model\"],\n        messages=[{\"role\": \"user\", \"content\": f\"Task for {agent['name']}\"}],\n        priority=priority\n    )\n\n    backend = response.get('_routing', {}).get('backend', 'unknown')\n    print(f\"{agent['name']} ({agent['model']}): {backend}\")\n</code></pre>"},{"location":"llama_cpp_guide/#model-profiles","title":"Model Profiles","text":"<p>SOLLOL uses model profiles to automatically determine routing strategy:</p>"},{"location":"llama_cpp_guide/#built-in-profiles","title":"Built-in Profiles","text":"<pre><code>MODEL_PROFILES = {\n    # Small models - Ollama\n    \"llama3.2\": {\n        \"parameter_count\": 3,\n        \"estimated_memory_gb\": 2,\n        \"requires_distributed\": False\n    },\n    \"phi\": {\n        \"parameter_count\": 3,\n        \"estimated_memory_gb\": 1.5,\n        \"requires_distributed\": False\n    },\n\n    # Medium models - Ollama (if fits)\n    \"llama3.1:8b\": {\n        \"parameter_count\": 8,\n        \"estimated_memory_gb\": 5,\n        \"requires_distributed\": False\n    },\n    \"codellama:13b\": {\n        \"parameter_count\": 13,\n        \"estimated_memory_gb\": 8,\n        \"requires_distributed\": False\n    },\n\n    # Large models - llama.cpp sharding\n    \"llama3.1:70b\": {\n        \"parameter_count\": 70,\n        \"estimated_memory_gb\": 40,\n        \"requires_distributed\": True\n    },\n    \"llama3.1:405b\": {\n        \"parameter_count\": 405,\n        \"estimated_memory_gb\": 240,\n        \"requires_distributed\": True\n    },\n    \"mixtral:8x7b\": {\n        \"parameter_count\": 47,  # MoE model\n        \"estimated_memory_gb\": 26,\n        \"requires_distributed\": True\n    }\n}\n</code></pre>"},{"location":"llama_cpp_guide/#custom-model-profiles","title":"Custom Model Profiles","text":"<p>Add your own model profiles:</p> <pre><code>from sollol.hybrid_router import MODEL_PROFILES\n\n# Add custom model\nMODEL_PROFILES[\"custom-70b\"] = {\n    \"parameter_count\": 70,\n    \"estimated_memory_gb\": 42,\n    \"requires_distributed\": True\n}\n\n# Now SOLLOL will route it to llama.cpp automatically\nrouter.route_request(\n    model=\"custom-70b\",\n    messages=[...]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#threshold-configuration","title":"Threshold Configuration","text":"<p>Adjust when sharding is used:</p> <pre><code>router = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    distributed_threshold_params=30,  # Shard models &gt; 30B parameters\n    num_rpc_backends=3\n)\n</code></pre>"},{"location":"llama_cpp_guide/#performance-optimization","title":"Performance &amp; Optimization","text":""},{"location":"llama_cpp_guide/#performance-characteristics","title":"Performance Characteristics","text":"<p>Startup Time: - First request: 2-5 minutes (model loading + layer distribution) - Subsequent requests: &lt;1 second (coordinator reuse)</p> <p>Inference Speed: - Local Ollama: ~20-40 tokens/sec (single GPU) - 2-node sharding: ~5-10 tokens/sec (~3-4\u00d7 slower) - 3-node sharding: ~3-7 tokens/sec (~5-6\u00d7 slower)</p> <p>Network Impact: <pre><code>Latency Impact:\n- &lt;1ms: Excellent (local network)\n- 1-10ms: Good (same datacenter)\n- 10-50ms: Acceptable (same region)\n- &gt;50ms: Poor (cross-region)\n</code></pre></p>"},{"location":"llama_cpp_guide/#optimization-tips","title":"Optimization Tips","text":"<p>1. Minimize RPC Hops <pre><code># Good: 2-3 backends (fewer network hops)\nrouter = HybridRouter(num_rpc_backends=2)\n\n# Avoid: 5+ backends (too many hops)\nrouter = HybridRouter(num_rpc_backends=6)\n</code></pre></p> <p>2. Use Fast Network <pre><code># Check network latency between machines\nping -c 10 192.168.1.11\n\n# Ensure &lt;10ms latency for good performance\n</code></pre></p> <p>3. Optimize Context Size <pre><code># Smaller context = faster inference\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[...],\n    max_tokens=512  # Limit response length\n)\n</code></pre></p> <p>4. Coordinator Reuse <pre><code># Coordinator stays loaded between requests\n# Subsequent requests are much faster\n\n# First request: 2-5 min (startup + inference)\nresponse1 = router.route_request(model=\"llama3.1:70b\", messages=[...])\n\n# Second request: &lt;1 min (inference only)\nresponse2 = router.route_request(model=\"llama3.1:70b\", messages=[...])\n</code></pre></p> <p>5. Monitor Performance <pre><code>response = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[...]\n)\n\n# Check routing metadata\nrouting = response.get('_routing', {})\nprint(f\"Backend: {routing.get('backend')}\")\nprint(f\"Duration: {routing.get('duration_ms')}ms\")\nprint(f\"Coordinator: {routing.get('coordinator_url')}\")\n</code></pre></p>"},{"location":"llama_cpp_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llama_cpp_guide/#issue-rpc-backends-not-found","title":"Issue: RPC Backends Not Found","text":"<p>Symptoms: <pre><code>\u26a0\ufe0f  No RPC backends found\n\ud83d\udce1 Model sharding disabled\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check RPC servers are running: <pre><code># List running RPC servers\nps aux | grep rpc-server\n\n# Should show:\n# ./build/bin/rpc-server -H 0.0.0.0 -p 50052\n</code></pre></p> </li> <li> <p>Verify network connectivity: <pre><code># Test port accessibility\nnc -zv 192.168.1.10 50052\n\n# Check firewall\nsudo ufw allow 50052\n</code></pre></p> </li> <li> <p>Enable auto-setup: <pre><code>router = HybridRouter(\n    enable_distributed=True,\n    auto_setup_rpc=True,  # Let SOLLOL build/start RPC servers\n    num_rpc_backends=3\n)\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-coordinator-startup-timeout","title":"Issue: Coordinator Startup Timeout","text":"<p>Symptoms: <pre><code>\ud83d\ude80 Starting llama.cpp coordinator...\n[waits 20+ minutes]\nTimeoutError: Coordinator failed to start\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Increase timeout: <pre><code>router = HybridRouter(\n    enable_distributed=True,\n    coordinator_timeout=1200,  # 20 minutes for 70B models\n    num_rpc_backends=3\n)\n</code></pre></p> </li> <li> <p>Check logs: <pre><code># View llama-server output\ntail -f /tmp/llama_coordinator_*.log\n</code></pre></p> </li> <li> <p>Verify GGUF exists: <pre><code>from sollol.ollama_gguf_resolver import OllamaGGUFResolver\n\nresolver = OllamaGGUFResolver()\ngguf_path = resolver.get_gguf_path(\"llama3.1:70b\")\nprint(f\"GGUF: {gguf_path}\")\n\n# Should print path like:\n# /usr/share/ollama/.ollama/models/blobs/sha256-abc123...\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-inference-timeout","title":"Issue: Inference Timeout","text":"<p>Symptoms: <pre><code>\u2705 Coordinator started successfully\n[inference request sent]\n[waits 5+ minutes]\nTimeoutError: Request timeout after 300s\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Increase request timeout: <pre><code>response = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[...],\n    timeout=600  # 10 minutes\n)\n</code></pre></p> </li> <li> <p>Check coordinator is responding: <pre><code># Test coordinator health\ncurl http://localhost:18080/health\n</code></pre></p> </li> <li> <p>Verify RPC communication: <pre><code># Check RPC backend logs\n# Look for layer assignment messages\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-coordinator-crashes-after-first-request","title":"Issue: Coordinator Crashes After First Request","text":"<p>Symptoms: <pre><code>\u2705 First inference successful\n[second request]\n\ud83d\ude80 Starting llama.cpp coordinator... (again)\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check process liveness: <pre><code># SOLLOL should detect dead processes\n# Look for: \"\u26a0\ufe0f  Coordinator process died!\"\n</code></pre></p> </li> <li> <p>Increase coordinator memory: <pre><code># Give coordinator more memory\nexport LLAMA_ARG_N_GPU_LAYERS=40\n</code></pre></p> </li> <li> <p>Check for OOM kills: <pre><code># Check system logs\ndmesg | grep -i \"out of memory\"\njournalctl -xe | grep llama\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-slow-performance","title":"Issue: Slow Performance","text":"<p>Symptoms: - Inference takes 30+ seconds per token - Network appears saturated</p> <p>Solutions:</p> <ol> <li> <p>Reduce number of backends: <pre><code># Fewer backends = fewer network hops\nrouter = HybridRouter(num_rpc_backends=2)  # Instead of 4\n</code></pre></p> </li> <li> <p>Check network latency: <pre><code>ping -c 100 192.168.1.11\n# Should be &lt;10ms average\n</code></pre></p> </li> <li> <p>Use local network: <pre><code># Ensure all machines are on same LAN\n# Avoid VPN or WAN connections\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#advanced-topics","title":"Advanced Topics","text":""},{"location":"llama_cpp_guide/#custom-gguf-paths","title":"Custom GGUF Paths","text":"<p>Override automatic GGUF detection:</p> <pre><code>from sollol import HybridRouter, OllamaPool\n\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    gguf_path=\"/path/to/custom/model.gguf\"\n)\n</code></pre>"},{"location":"llama_cpp_guide/#multiple-coordinators","title":"Multiple Coordinators","text":"<p>Run different models simultaneously:</p> <pre><code># Not currently supported - coordinators are per-HybridRouter\n# Workaround: Use separate HybridRouter instances\n\nrouter_70b = HybridRouter(\n    enable_distributed=True,\n    model_filter=[\"llama3.1:70b\"]\n)\n\nrouter_405b = HybridRouter(\n    enable_distributed=True,\n    model_filter=[\"llama3.1:405b\"]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#layer-distribution-strategies","title":"Layer Distribution Strategies","text":"<p>Coming soon: Custom layer distribution</p> <pre><code># Future feature\nrouter = HybridRouter(\n    enable_distributed=True,\n    layer_strategy=\"memory_aware\",  # Distribute based on VRAM\n    # or \"even\" for equal distribution\n)\n</code></pre>"},{"location":"llama_cpp_guide/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":"<p>Get detailed metrics:</p> <pre><code>stats = router.get_stats()\n\nprint(f\"Distributed requests: {stats.get('distributed_requests', 0)}\")\nprint(f\"Coordinator uptime: {stats.get('coordinator_uptime_seconds', 0)}s\")\nprint(f\"Active RPC backends: {stats.get('active_rpc_backends', 0)}\")\n</code></pre>"},{"location":"llama_cpp_guide/#see-also","title":"See Also","text":"<ul> <li>ARCHITECTURE.md - SOLLOL architecture overview</li> <li>HybridRouter API - HybridRouter documentation</li> <li>llama.cpp GitHub - llama.cpp project</li> <li>Integration Examples - More usage examples</li> </ul>"},{"location":"llama_cpp_guide/#summary","title":"Summary","text":"<p>SOLLOL's llama.cpp integration makes model sharding accessible:</p> <p>\u2705 Easy Setup - Auto-discovery and auto-setup \u2705 Intelligent Routing - Automatic backend selection \u2705 GGUF Extraction - No manual file management \u2705 Hybrid Operation - Small models stay fast, large models become possible \u2705 Production Ready - Coordinator reuse, health checking, failover</p> <p>Quick Start: <pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    auto_setup_rpc=True,\n    num_rpc_backends=3\n)\n\n# Just use it - SOLLOL handles the rest\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre></p> <p>That's it! \ud83d\ude80</p>"},{"location":"enterprise/roadmap/","title":"Enterprise Roadmap","text":""},{"location":"enterprise/roadmap/#future-enterprise-opportunities","title":"Future Enterprise Opportunities","text":"<p>The free version includes everything you need for production deployments. These advanced features are available through custom development partnerships and sponsorship.</p>"},{"location":"enterprise/roadmap/#available-enterprise-features","title":"Available Enterprise Features","text":""},{"location":"enterprise/roadmap/#ray-train-integration","title":"\ud83d\udd27 Ray Train Integration","text":"<p>Distributed model fine-tuning across GPU clusters</p> <p>Train custom LLMs on your infrastructure with distributed training:</p> <ul> <li>Multi-node GPU orchestration</li> <li>Automatic checkpointing and recovery</li> <li>Distributed data loading</li> <li>Training metrics and visualization</li> <li>Integration with your existing models</li> </ul> <p>Use Case: Organizations training custom models on proprietary data</p>"},{"location":"enterprise/roadmap/#multi-region-orchestration","title":"\ud83c\udf10 Multi-Region Orchestration","text":"<p>Global load balancing with geo-aware routing</p> <p>Deploy SOLLOL across multiple regions with intelligent geo-routing:</p> <ul> <li>Latency-based routing (&lt;100ms worldwide)</li> <li>Regional failover and disaster recovery</li> <li>Data sovereignty compliance</li> <li>Cross-region traffic management</li> <li>Global health monitoring</li> </ul> <p>Use Case: Worldwide SaaS platforms requiring low-latency AI inference</p>"},{"location":"enterprise/roadmap/#advanced-analytics-suite","title":"\ud83d\udcca Advanced Analytics Suite","text":"<p>ML-powered capacity planning and cost optimization</p> <p>Predictive analytics for optimal resource utilization:</p> <ul> <li>ML-based demand forecasting</li> <li>Automated capacity recommendations</li> <li>Cost optimization algorithms</li> <li>Budget alerts and reporting</li> <li>Usage trend analysis</li> </ul> <p>Use Case: Enterprises optimizing cloud spend and resource allocation</p>"},{"location":"enterprise/roadmap/#enterprise-sso-integration","title":"\ud83d\udd10 Enterprise SSO Integration","text":"<p>SAML, OAuth2, LDAP, Active Directory</p> <p>Enterprise identity management integration:</p> <ul> <li>Single sign-on (SAML 2.0)</li> <li>OAuth2/OIDC providers</li> <li>LDAP/Active Directory sync</li> <li>Role mapping and provisioning</li> <li>Audit logging</li> </ul> <p>Use Case: Large organizations with existing identity infrastructure</p>"},{"location":"enterprise/roadmap/#custom-routing-engines","title":"\ud83c\udfaf Custom Routing Engines","text":"<p>Bespoke algorithms for specialized workloads</p> <p>Industry-specific routing optimizations:</p> <ul> <li>Custom scoring algorithms</li> <li>Domain-specific task detection</li> <li>Specialized performance metrics</li> <li>Workflow-aware routing</li> <li>Integration with internal systems</li> </ul> <p>Use Case: Companies with unique workload patterns or compliance requirements</p>"},{"location":"enterprise/roadmap/#sla-guarantees","title":"\ud83d\udee1\ufe0f SLA Guarantees","text":"<p>99.9%+ uptime with priority support</p> <p>Enterprise-grade reliability and support:</p> <ul> <li>99.9% uptime SLA</li> <li>4-hour incident response</li> <li>Dedicated support engineer</li> <li>Quarterly architecture reviews</li> <li>Custom runbooks and documentation</li> </ul> <p>Use Case: Mission-critical production systems requiring guaranteed uptime</p>"},{"location":"enterprise/roadmap/#dedicated-support","title":"\ud83d\udcde Dedicated Support","text":"<p>Hands-on partnership and architecture reviews</p> <p>Direct access to SOLLOL experts:</p> <ul> <li>Private Slack channel</li> <li>Weekly video calls</li> <li>Architecture design sessions</li> <li>Performance optimization reviews</li> <li>On-call engineering support</li> </ul> <p>Use Case: Teams needing expert guidance for complex deployments</p>"},{"location":"enterprise/roadmap/#custom-development","title":"\ud83c\udfd7\ufe0f Custom Development","text":"<p>Tailored features for your infrastructure</p> <p>Bespoke development services:</p> <ul> <li>Custom integrations (monitoring, logging, etc.)</li> <li>Proprietary protocol support</li> <li>Internal tool integration</li> <li>Feature development</li> <li>Migration assistance</li> </ul> <p>Use Case: Organizations with unique technical requirements</p>"},{"location":"enterprise/roadmap/#why-sponsorship","title":"Why Sponsorship?","text":"<p>Each enterprise feature involves:</p> <ul> <li>Months of development time - Complex features requiring significant engineering effort</li> <li>Enterprise integrations - Testing with SAML providers, LDAP servers, cloud platforms</li> <li>Ongoing maintenance - Security patches, compatibility updates, bug fixes</li> <li>Multi-environment testing - Validation across different infrastructure setups</li> <li>Comprehensive documentation - Architecture guides, deployment runbooks, training materials</li> </ul>"},{"location":"enterprise/roadmap/#get-started","title":"Get Started","text":""},{"location":"enterprise/roadmap/#engagement-process","title":"Engagement Process","text":"<ol> <li>Discovery Call - Discuss your requirements and use case</li> <li>Proposal - Detailed scope, timeline, and pricing</li> <li>Development - Fixed-price or retainer-based engagement</li> <li>Delivery - Feature implementation with documentation</li> <li>Support - Ongoing maintenance and updates</li> </ol>"},{"location":"enterprise/roadmap/#contact","title":"Contact","text":"<ul> <li>GitHub Sponsors: Sponsor SOLLOL Development</li> <li>Discussions: Start a conversation</li> <li>Email: Open a discussion for direct contact</li> </ul>"},{"location":"enterprise/roadmap/#pricing-models","title":"Pricing Models","text":"<p>Fixed-Price Projects - Defined scope and deliverables - Clear timeline and milestones - Typical range: $50k - $200k per feature</p> <p>Retainer Agreements - Monthly commitment for ongoing development - Flexible scope and priorities - Includes support and maintenance - Typical range: $20k - $50k/month</p> <p>Custom Packages - Combination of features - Volume discounts available - Long-term partnerships</p>"},{"location":"enterprise/roadmap/#existing-clients","title":"Existing Clients","text":"<p>Coming soon - enterprise client case studies and testimonials</p>"},{"location":"enterprise/roadmap/#faq","title":"FAQ","text":"<p>Q: Can I contribute enterprise features as open source?</p> <p>A: Absolutely! If you develop a feature and want to contribute it back, we welcome PRs. Enterprise features are those requiring sponsored development effort.</p> <p>Q: How long does custom development take?</p> <p>A: Typically 3-6 months per major feature, depending on complexity and integration requirements.</p> <p>Q: Do you offer proof-of-concept work?</p> <p>A: Yes, we can start with a small PoC (2-4 weeks) to validate feasibility before full development.</p> <p>Q: What happens if my needs change?</p> <p>A: Retainer agreements offer flexibility to adjust priorities. Fixed-price projects have change order processes.</p> <p>Ready to discuss enterprise features? Start a conversation \u2192</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get SOLLOL running in 5 minutes with Docker Compose.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>8GB+ RAM recommended</li> <li>(Optional) GPU for optimal performance</li> </ul>"},{"location":"getting-started/quick-start/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/BenevolentJoker-JohnL/SOLLOL.git\ncd SOLLOL\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-start-the-stack","title":"Step 2: Start the Stack","text":"<pre><code># Start SOLLOL + 3 Ollama nodes + Prometheus + Grafana\ndocker-compose up -d\n\n# Check status\ndocker-compose ps\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-pull-a-model","title":"Step 3: Pull a Model","text":"<pre><code># Pull llama3.2 on all nodes\ndocker exec sollol-ollama-node-1-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-2-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-3-1 ollama pull llama3.2\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-test-the-setup","title":"Step 4: Test the Setup","text":"<pre><code># Send a test request to SOLLOL (drop-in replacement on port 11434)\ncurl -X POST http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n\n# Or use the standard Ollama API (SOLLOL is transparent!)\nexport OLLAMA_HOST=localhost:11434\nollama run llama3.2 \"Hello!\"\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-view-the-dashboard","title":"Step 5: View the Dashboard","text":"<p>Open your browser:</p> <ul> <li>SOLLOL Dashboard: http://localhost:11434/dashboard.html</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Prometheus: http://localhost:9091</li> </ul>"},{"location":"getting-started/quick-start/#using-the-python-sdk","title":"Using the Python SDK","text":"<pre><code>from sollol import connect\n\n# Connect to SOLLOL (drop-in replacement - same port as Ollama!)\nsollol = connect(\"http://localhost:11434\")\n\n# Chat with intelligent routing\nresponse = sollol.chat(\n    \"Explain quantum computing\",\n    priority=8  # High priority = faster nodes\n)\n\nprint(response['message']['content'])\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize SOLLOL settings</li> <li>Architecture - Understand how it works</li> <li>Deployment - Deploy to production</li> <li>Benchmarks - Run performance tests</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#ollama-nodes-not-responding","title":"Ollama nodes not responding","text":"<pre><code># Check logs\ndocker-compose logs sollol\ndocker-compose logs ollama-node-1\n\n# Restart a node\ndocker-compose restart ollama-node-1\n</code></pre>"},{"location":"getting-started/quick-start/#port-conflicts","title":"Port conflicts","text":"<p>If port 11434 is already in use (e.g., you have Ollama running), stop it first:</p> <pre><code># Stop standalone Ollama (if running)\npkill ollama\n\n# Or change SOLLOL's port in docker-compose.yml\nports:\n  - \"11435:11434\"  # Change external port\n</code></pre>"},{"location":"getting-started/quick-start/#gpu-not-detected","title":"GPU not detected","text":"<p>Ensure NVIDIA Container Toolkit is installed:</p> <pre><code># Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre> <p>Then uncomment GPU sections in <code>docker-compose.yml</code>.</p>"},{"location":"getting-started/quick-start/#support","title":"Support","text":"<p>Need help? Open an issue on GitHub.</p>"}]}
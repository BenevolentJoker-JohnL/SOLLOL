{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SOLLOL Documentation","text":"<p>Production-Ready Intelligent Load Balancing for Ollama Clusters</p> <p>Quick Start View on GitHub</p>"},{"location":"#what-is-sollol","title":"What is SOLLOL?","text":"<p>SOLLOL transforms multiple Ollama nodes into a unified, intelligent AI inference cluster.</p> <p>Instead of manually managing multiple Ollama instances or using simple round-robin load balancing, SOLLOL analyzes each request's requirements and automatically routes it to the optimal node based on:</p> <ul> <li>Task complexity (embedding vs generation vs analysis)</li> <li>Resource availability (GPU memory, CPU load)</li> <li>Real-time performance (latency, success rate)</li> <li>Historical patterns (adaptive learning from past executions)</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Intelligent Routing</p> <p>Context-aware analysis routes requests to optimal nodes using multi-factor scoring and adaptive learning.</p> <p> Learn more</p> </li> <li> <p> Priority Queue</p> <p>10-level priority system with age-based fairness ensures critical tasks get resources without starvation.</p> <p> Learn more</p> </li> <li> <p> Auto Failover</p> <p>3 retry attempts with exponential backoff and health monitoring ensure zero-downtime operation.</p> <p> Learn more</p> </li> <li> <p> Observability</p> <p>Real-time dashboard with Prometheus metrics and routing transparency for complete visibility.</p> <p> Learn more</p> </li> <li> <p> High Performance</p> <p>Ray actors and Dask batch processing deliver &lt;10ms routing overhead with 40-60% throughput gains.</p> <p> Learn more</p> </li> <li> <p> Enterprise Security</p> <p>SHA-256 API keys with RBAC permissions and per-key rate limiting for production deployments.</p> <p> Learn more</p> </li> </ul>"},{"location":"#performance-impact","title":"Performance Impact","text":"Metric Expected Improvement Avg Latency -30-40% (context-aware routing to optimal nodes) P95 Latency -40-50% (avoiding overloaded nodes) Success Rate +2-4pp (automatic failover and retry) Throughput +40-60% (better resource utilization) GPU Utilization +50-80% (intelligent task-to-GPU matching) <p>Production Ready</p> <p>SOLLOL is fully functional and production-ready. No artificial limits, no feature gates.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Architecture Overview - Understand the system design</li> <li>Deployment Guide - Deploy with Docker or Kubernetes</li> <li>Benchmarks - See performance methodology and results</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions or share ideas</li> <li>Contributing: See contribution guidelines</li> <li>Enterprise: Explore enterprise features</li> </ul> <p>Built with  by BenevolentJoker-JohnL</p> <p>GitHub Sponsor</p>"},{"location":"layer_partitioning/","title":"Layer Partitioning for Large Models","text":"<p>SOLLOL now supports layer partitioning - the ability to split large models (70B+) across multiple nodes for distributed inference.</p>"},{"location":"layer_partitioning/#overview","title":"Overview","text":"<p>What it does: - Splits models too large for a single GPU across multiple nodes - Each node loads specific layers (e.g., node1: layers 0-39, node2: layers 40-79) - Coordinates inference across nodes automatically - Provides both vertical scaling (bigger models) and horizontal scaling (more throughput)</p> <p>When to use it: - Models larger than your single-node GPU memory (Llama-70B, Mixtral-8x7B, etc.) - You have multiple GPUs across different machines - You want to run models that wouldn't fit otherwise</p>"},{"location":"layer_partitioning/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    SOLLOL Gateway                    \u2502\n\u2502                                                      \u2502\n\u2502  Request for llama2:70b                              \u2502\n\u2502    \u2193                                                 \u2502\n\u2502  Routing Decision:                                   \u2502\n\u2502    - Small model (llama3.2) \u2192 Individual node        \u2502\n\u2502    - Large model (llama2:70b) \u2192 Cluster              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                             \u2502\n        \u25bc                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Individual     \u2502         \u2502   Node Cluster   \u2502\n\u2502      Node        \u2502         \u2502   (llama2:70b)   \u2502\n\u2502                  \u2502         \u2502                  \u2502\n\u2502  \u2022 llama3.2      \u2502         \u2502  Node 1:         \u2502\n\u2502  \u2022 phi           \u2502         \u2502    Layers 0-39   \u2502\n\u2502  \u2022 codellama     \u2502         \u2502                  \u2502\n\u2502                  \u2502         \u2502  Node 2:         \u2502\n\u2502  Full model      \u2502         \u2502    Layers 40-79  \u2502\n\u2502  on single GPU   \u2502         \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  Distributed     \u2502\n                             \u2502  inference       \u2502\n                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"layer_partitioning/#quick-start","title":"Quick Start","text":""},{"location":"layer_partitioning/#1-add-nodes-to-registry","title":"1. Add Nodes to Registry","text":"<pre><code>from sollol.registry import NodeRegistry\n\nregistry = NodeRegistry()\n\n# Add individual nodes\nregistry.add_node(\"http://192.168.1.10:11434\", name=\"gpu-node-1\")\nregistry.add_node(\"http://192.168.1.11:11434\", name=\"gpu-node-2\")\nregistry.add_node(\"http://192.168.1.12:11434\", name=\"gpu-node-3\")\n</code></pre>"},{"location":"layer_partitioning/#2-create-cluster-for-large-model","title":"2. Create Cluster for Large Model","text":"<pre><code># Create cluster for Llama-70B across 2 nodes\ncluster = registry.create_cluster(\n    name=\"llama70b-cluster\",\n    node_urls=[\n        \"http://192.168.1.10:11434\",\n        \"http://192.168.1.11:11434\"\n    ],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"even\"  # or \"memory_aware\"\n)\n\n# Output:\n# \ud83d\udce6 Created cluster 'llama70b-cluster' with 2 nodes for llama2:70b\n#    Node 1: layers 0-39 (40 layers)\n#    Node 2: layers 40-79 (40 layers)\n</code></pre>"},{"location":"layer_partitioning/#3-use-smart-routing","title":"3. Use Smart Routing","text":"<pre><code># Get best worker for model (automatically selects cluster for large models)\nworker = registry.get_worker_for_model(\"llama2:70b\")\n\nif isinstance(worker, NodeCluster):\n    print(f\"Using cluster: {worker.name}\")\n    result = await worker.generate(\"Explain quantum computing\")\nelse:\n    print(f\"Using single node: {worker.name}\")\n</code></pre>"},{"location":"layer_partitioning/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"layer_partitioning/#even-distribution-default","title":"Even Distribution (Default)","text":"<p>Splits layers evenly across nodes:</p> <pre><code>cluster = registry.create_cluster(\n    name=\"my-cluster\",\n    node_urls=[...],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"even\"\n)\n\n# 80 layers across 2 nodes:\n# Node 1: 40 layers (0-39)\n# Node 2: 40 layers (40-79)\n</code></pre>"},{"location":"layer_partitioning/#memory-aware-distribution","title":"Memory-Aware Distribution","text":"<p>Allocates layers proportionally to available memory:</p> <pre><code>cluster = registry.create_cluster(\n    name=\"my-cluster\",\n    node_urls=[...],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"memory_aware\"\n)\n\n# If Node 1 has 48GB and Node 2 has 24GB:\n# Node 1: 53 layers (0-52)  # 66% of layers\n# Node 2: 27 layers (53-79)  # 33% of layers\n</code></pre>"},{"location":"layer_partitioning/#supported-models","title":"Supported Models","text":""},{"location":"layer_partitioning/#large-models-require-partitioning","title":"Large Models (Require Partitioning)","text":"<ul> <li>llama2:70b - 80 layers, ~36GB memory</li> <li>llama3:70b - 80 layers, ~36GB memory</li> <li>mixtral:8x7b - 32 layers (MoE), ~26GB memory</li> </ul>"},{"location":"layer_partitioning/#small-models-single-node","title":"Small Models (Single Node)","text":"<ul> <li>llama3.2 - 32 layers, ~2GB memory</li> <li>phi - 32 layers, ~1.5GB memory</li> <li>codellama:7b - 32 layers, ~4GB memory</li> </ul>"},{"location":"layer_partitioning/#adding-custom-models","title":"Adding Custom Models","text":"<p>Edit <code>sollol/node_cluster.py</code> to add model specs:</p> <pre><code>MODEL_SPECS = {\n    \"your-model:70b\": ModelSpec(\n        name=\"your-model:70b\",\n        total_layers=80,\n        memory_per_layer_mb=450,\n        min_memory_mb=4096\n    ),\n}\n</code></pre>"},{"location":"layer_partitioning/#health-checking","title":"Health Checking","text":"<p>Clusters require ALL nodes to be healthy:</p> <pre><code># Check cluster health\nis_healthy = await cluster.health_check()\n\nif not is_healthy:\n    print(f\"Cluster unhealthy - nodes down: {[n.url for n in cluster.nodes if not n.is_healthy]}\")\n\n# Check all clusters\ncluster_health = await registry.health_check_clusters()\n</code></pre>"},{"location":"layer_partitioning/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom sollol.registry import NodeRegistry\n\nasync def main():\n    # Setup registry\n    registry = NodeRegistry()\n\n    # Discover nodes on network\n    registry.discover_nodes(cidr=\"192.168.1.0/24\")\n\n    # Create cluster for large model\n    if len(registry.get_healthy_nodes()) &gt;= 2:\n        cluster = registry.create_cluster(\n            name=\"llama70b\",\n            node_urls=[\n                registry.get_healthy_nodes()[0].url,\n                registry.get_healthy_nodes()[1].url\n            ],\n            model=\"llama2:70b\"\n        )\n\n        # Run inference\n        result = await cluster.generate(\n            prompt=\"Write a detailed explanation of quantum entanglement\",\n            options={\"temperature\": 0.7}\n        )\n\n        print(result['response'])\n        print(f\"\\nCluster info: {result['_cluster']}\")\n\n    # Small models use individual nodes\n    worker = registry.get_worker_for_model(\"llama3.2\")\n    print(f\"Small model routed to: {worker.name}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"layer_partitioning/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"layer_partitioning/#latency-trade-offs","title":"Latency Trade-offs","text":"<ul> <li>Single Node: Fastest (no inter-node communication)</li> <li>Cluster (2 nodes): ~10-20% slower due to coordination overhead</li> <li>Cluster (3+ nodes): Additional latency per node</li> </ul>"},{"location":"layer_partitioning/#throughput-benefits","title":"Throughput Benefits","text":"<ul> <li>Load Balancing: Small models spread across available nodes</li> <li>Large Model Access: Run models impossible on single GPU</li> <li>Mixed Workloads: Clusters handle 70B while other nodes serve 7B/13B</li> </ul>"},{"location":"layer_partitioning/#optimal-configurations","title":"Optimal Configurations","text":"<p>2-Node Cluster (Recommended for 70B models) <pre><code>GPU 1: 24GB - Layers 0-39 of Llama-70B\nGPU 2: 24GB - Layers 40-79 of Llama-70B\n</code></pre></p> <p>3-Node Cluster (For extremely large or multiple 70B models) <pre><code>GPU 1: 24GB - Layers 0-26\nGPU 2: 24GB - Layers 27-53\nGPU 3: 24GB - Layers 54-79\n</code></pre></p>"},{"location":"layer_partitioning/#limitations","title":"Limitations","text":"<p>Current implementation: - \u2705 Layer partitioning logic and cluster management - \u2705 Health checking and failover - \u2705 Smart routing (large \u2192 cluster, small \u2192 single) - \u26a0\ufe0f  Inter-node communication (basic implementation) - \u26a0\ufe0f  Requires Ollama layer partitioning support (WIP upstream)</p> <p>Future enhancements: 1. gRPC for faster inter-node communication 2. Session affinity for multi-turn conversations 3. Dynamic layer rebalancing based on load 4. Automatic cluster creation on demand</p>"},{"location":"layer_partitioning/#comparison-sollol-vs-ollol","title":"Comparison: SOLLOL vs OLLOL","text":"Feature SOLLOL OLLOL (K2/olol) Load balancing \u2705 Advanced \u2705 Basic Layer partitioning \u2705 New \u2705 Existing Health scoring \u2705 Performance-based \u2705 Simple ping Auto-discovery \u2705 CIDR scanning \u2705 Broadcast Intelligent routing \u2705 Task-aware \u274c Priority queuing \u2705 \u274c Observability \u2705 Dashboard \u274c <p>SOLLOL now provides both capabilities from OLLOL: - Load balancing across independent workers (existing) - Layer partitioning for large models (new)</p>"},{"location":"layer_partitioning/#see-also","title":"See Also","text":"<ul> <li>Node Registry Documentation</li> <li>Intelligent Routing</li> <li>Network Discovery</li> </ul>"},{"location":"enterprise/roadmap/","title":"Enterprise Roadmap","text":""},{"location":"enterprise/roadmap/#future-enterprise-opportunities","title":"Future Enterprise Opportunities","text":"<p>The free version includes everything you need for production deployments. These advanced features are available through custom development partnerships and sponsorship.</p>"},{"location":"enterprise/roadmap/#available-enterprise-features","title":"Available Enterprise Features","text":""},{"location":"enterprise/roadmap/#ray-train-integration","title":"\ud83d\udd27 Ray Train Integration","text":"<p>Distributed model fine-tuning across GPU clusters</p> <p>Train custom LLMs on your infrastructure with distributed training:</p> <ul> <li>Multi-node GPU orchestration</li> <li>Automatic checkpointing and recovery</li> <li>Distributed data loading</li> <li>Training metrics and visualization</li> <li>Integration with your existing models</li> </ul> <p>Use Case: Organizations training custom models on proprietary data</p>"},{"location":"enterprise/roadmap/#multi-region-orchestration","title":"\ud83c\udf10 Multi-Region Orchestration","text":"<p>Global load balancing with geo-aware routing</p> <p>Deploy SOLLOL across multiple regions with intelligent geo-routing:</p> <ul> <li>Latency-based routing (&lt;100ms worldwide)</li> <li>Regional failover and disaster recovery</li> <li>Data sovereignty compliance</li> <li>Cross-region traffic management</li> <li>Global health monitoring</li> </ul> <p>Use Case: Worldwide SaaS platforms requiring low-latency AI inference</p>"},{"location":"enterprise/roadmap/#advanced-analytics-suite","title":"\ud83d\udcca Advanced Analytics Suite","text":"<p>ML-powered capacity planning and cost optimization</p> <p>Predictive analytics for optimal resource utilization:</p> <ul> <li>ML-based demand forecasting</li> <li>Automated capacity recommendations</li> <li>Cost optimization algorithms</li> <li>Budget alerts and reporting</li> <li>Usage trend analysis</li> </ul> <p>Use Case: Enterprises optimizing cloud spend and resource allocation</p>"},{"location":"enterprise/roadmap/#enterprise-sso-integration","title":"\ud83d\udd10 Enterprise SSO Integration","text":"<p>SAML, OAuth2, LDAP, Active Directory</p> <p>Enterprise identity management integration:</p> <ul> <li>Single sign-on (SAML 2.0)</li> <li>OAuth2/OIDC providers</li> <li>LDAP/Active Directory sync</li> <li>Role mapping and provisioning</li> <li>Audit logging</li> </ul> <p>Use Case: Large organizations with existing identity infrastructure</p>"},{"location":"enterprise/roadmap/#custom-routing-engines","title":"\ud83c\udfaf Custom Routing Engines","text":"<p>Bespoke algorithms for specialized workloads</p> <p>Industry-specific routing optimizations:</p> <ul> <li>Custom scoring algorithms</li> <li>Domain-specific task detection</li> <li>Specialized performance metrics</li> <li>Workflow-aware routing</li> <li>Integration with internal systems</li> </ul> <p>Use Case: Companies with unique workload patterns or compliance requirements</p>"},{"location":"enterprise/roadmap/#sla-guarantees","title":"\ud83d\udee1\ufe0f SLA Guarantees","text":"<p>99.9%+ uptime with priority support</p> <p>Enterprise-grade reliability and support:</p> <ul> <li>99.9% uptime SLA</li> <li>4-hour incident response</li> <li>Dedicated support engineer</li> <li>Quarterly architecture reviews</li> <li>Custom runbooks and documentation</li> </ul> <p>Use Case: Mission-critical production systems requiring guaranteed uptime</p>"},{"location":"enterprise/roadmap/#dedicated-support","title":"\ud83d\udcde Dedicated Support","text":"<p>Hands-on partnership and architecture reviews</p> <p>Direct access to SOLLOL experts:</p> <ul> <li>Private Slack channel</li> <li>Weekly video calls</li> <li>Architecture design sessions</li> <li>Performance optimization reviews</li> <li>On-call engineering support</li> </ul> <p>Use Case: Teams needing expert guidance for complex deployments</p>"},{"location":"enterprise/roadmap/#custom-development","title":"\ud83c\udfd7\ufe0f Custom Development","text":"<p>Tailored features for your infrastructure</p> <p>Bespoke development services:</p> <ul> <li>Custom integrations (monitoring, logging, etc.)</li> <li>Proprietary protocol support</li> <li>Internal tool integration</li> <li>Feature development</li> <li>Migration assistance</li> </ul> <p>Use Case: Organizations with unique technical requirements</p>"},{"location":"enterprise/roadmap/#why-sponsorship","title":"Why Sponsorship?","text":"<p>Each enterprise feature involves:</p> <ul> <li>Months of development time - Complex features requiring significant engineering effort</li> <li>Enterprise integrations - Testing with SAML providers, LDAP servers, cloud platforms</li> <li>Ongoing maintenance - Security patches, compatibility updates, bug fixes</li> <li>Multi-environment testing - Validation across different infrastructure setups</li> <li>Comprehensive documentation - Architecture guides, deployment runbooks, training materials</li> </ul>"},{"location":"enterprise/roadmap/#get-started","title":"Get Started","text":""},{"location":"enterprise/roadmap/#engagement-process","title":"Engagement Process","text":"<ol> <li>Discovery Call - Discuss your requirements and use case</li> <li>Proposal - Detailed scope, timeline, and pricing</li> <li>Development - Fixed-price or retainer-based engagement</li> <li>Delivery - Feature implementation with documentation</li> <li>Support - Ongoing maintenance and updates</li> </ol>"},{"location":"enterprise/roadmap/#contact","title":"Contact","text":"<ul> <li>GitHub Sponsors: Sponsor SOLLOL Development</li> <li>Discussions: Start a conversation</li> <li>Email: Open a discussion for direct contact</li> </ul>"},{"location":"enterprise/roadmap/#pricing-models","title":"Pricing Models","text":"<p>Fixed-Price Projects - Defined scope and deliverables - Clear timeline and milestones - Typical range: $50k - $200k per feature</p> <p>Retainer Agreements - Monthly commitment for ongoing development - Flexible scope and priorities - Includes support and maintenance - Typical range: $20k - $50k/month</p> <p>Custom Packages - Combination of features - Volume discounts available - Long-term partnerships</p>"},{"location":"enterprise/roadmap/#existing-clients","title":"Existing Clients","text":"<p>Coming soon - enterprise client case studies and testimonials</p>"},{"location":"enterprise/roadmap/#faq","title":"FAQ","text":"<p>Q: Can I contribute enterprise features as open source?</p> <p>A: Absolutely! If you develop a feature and want to contribute it back, we welcome PRs. Enterprise features are those requiring sponsored development effort.</p> <p>Q: How long does custom development take?</p> <p>A: Typically 3-6 months per major feature, depending on complexity and integration requirements.</p> <p>Q: Do you offer proof-of-concept work?</p> <p>A: Yes, we can start with a small PoC (2-4 weeks) to validate feasibility before full development.</p> <p>Q: What happens if my needs change?</p> <p>A: Retainer agreements offer flexibility to adjust priorities. Fixed-price projects have change order processes.</p> <p>Ready to discuss enterprise features? Start a conversation \u2192</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get SOLLOL running in 5 minutes with Docker Compose.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>8GB+ RAM recommended</li> <li>(Optional) GPU for optimal performance</li> </ul>"},{"location":"getting-started/quick-start/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/BenevolentJoker-JohnL/SOLLOL.git\ncd SOLLOL\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-start-the-stack","title":"Step 2: Start the Stack","text":"<pre><code># Start SOLLOL + 3 Ollama nodes + Prometheus + Grafana\ndocker-compose up -d\n\n# Check status\ndocker-compose ps\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-pull-a-model","title":"Step 3: Pull a Model","text":"<pre><code># Pull llama3.2 on all nodes\ndocker exec sollol-ollama-node-1-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-2-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-3-1 ollama pull llama3.2\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-test-the-setup","title":"Step 4: Test the Setup","text":"<pre><code># Send a test request to SOLLOL (drop-in replacement on port 11434)\ncurl -X POST http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n\n# Or use the standard Ollama API (SOLLOL is transparent!)\nexport OLLAMA_HOST=localhost:11434\nollama run llama3.2 \"Hello!\"\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-view-the-dashboard","title":"Step 5: View the Dashboard","text":"<p>Open your browser:</p> <ul> <li>SOLLOL Dashboard: http://localhost:11434/dashboard.html</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Prometheus: http://localhost:9091</li> </ul>"},{"location":"getting-started/quick-start/#using-the-python-sdk","title":"Using the Python SDK","text":"<pre><code>from sollol import connect\n\n# Connect to SOLLOL (drop-in replacement - same port as Ollama!)\nsollol = connect(\"http://localhost:11434\")\n\n# Chat with intelligent routing\nresponse = sollol.chat(\n    \"Explain quantum computing\",\n    priority=8  # High priority = faster nodes\n)\n\nprint(response['message']['content'])\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize SOLLOL settings</li> <li>Architecture - Understand how it works</li> <li>Deployment - Deploy to production</li> <li>Benchmarks - Run performance tests</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#ollama-nodes-not-responding","title":"Ollama nodes not responding","text":"<pre><code># Check logs\ndocker-compose logs sollol\ndocker-compose logs ollama-node-1\n\n# Restart a node\ndocker-compose restart ollama-node-1\n</code></pre>"},{"location":"getting-started/quick-start/#port-conflicts","title":"Port conflicts","text":"<p>If port 11434 is already in use (e.g., you have Ollama running), stop it first:</p> <pre><code># Stop standalone Ollama (if running)\npkill ollama\n\n# Or change SOLLOL's port in docker-compose.yml\nports:\n  - \"11435:11434\"  # Change external port\n</code></pre>"},{"location":"getting-started/quick-start/#gpu-not-detected","title":"GPU not detected","text":"<p>Ensure NVIDIA Container Toolkit is installed:</p> <pre><code># Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre> <p>Then uncomment GPU sections in <code>docker-compose.yml</code>.</p>"},{"location":"getting-started/quick-start/#support","title":"Support","text":"<p>Need help? Open an issue on GitHub.</p>"}]}
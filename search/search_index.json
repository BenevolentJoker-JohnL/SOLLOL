{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SOLLOL Documentation","text":"<p>Production-Ready Intelligent Load Balancing for Ollama Clusters</p> <p>Quick Start View on GitHub</p>"},{"location":"#what-is-sollol","title":"What is SOLLOL?","text":"<p>SOLLOL transforms multiple Ollama nodes into a unified, intelligent AI inference cluster.</p> <p>Instead of manually managing multiple Ollama instances or using simple round-robin load balancing, SOLLOL analyzes each request's requirements and automatically routes it to the optimal node based on:</p> <ul> <li>Task complexity (embedding vs generation vs analysis)</li> <li>Resource availability (GPU memory, CPU load)</li> <li>Real-time performance (latency, success rate)</li> <li>Historical patterns (adaptive learning from past executions)</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Intelligent Routing</p> <p>Context-aware analysis routes requests to optimal nodes using multi-factor scoring and adaptive learning.</p> <p> Learn more</p> </li> <li> <p> Priority Queue</p> <p>10-level priority system with age-based fairness ensures critical tasks get resources without starvation.</p> <p> Learn more</p> </li> <li> <p> Auto Failover</p> <p>3 retry attempts with exponential backoff and health monitoring ensure zero-downtime operation.</p> <p> Learn more</p> </li> <li> <p> Observability</p> <p>Real-time dashboard with Prometheus metrics and routing transparency for complete visibility.</p> <p> Learn more</p> </li> <li> <p> High Performance</p> <p>Ray actors and Dask batch processing deliver &lt;10ms routing overhead with 40-60% throughput gains.</p> <p> Learn more</p> </li> <li> <p> Enterprise Security</p> <p>SHA-256 API keys with RBAC permissions and per-key rate limiting for production deployments.</p> <p> Learn more</p> </li> </ul>"},{"location":"#performance-impact","title":"Performance Impact","text":"Metric Expected Improvement Avg Latency -30-40% (context-aware routing to optimal nodes) P95 Latency -40-50% (avoiding overloaded nodes) Success Rate +2-4pp (automatic failover and retry) Throughput +40-60% (better resource utilization) GPU Utilization +50-80% (intelligent task-to-GPU matching) <p>Production Ready</p> <p>SOLLOL is fully functional and production-ready. No artificial limits, no feature gates.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Architecture Overview - Understand the system design</li> <li>Deployment Guide - Deploy with Docker or Kubernetes</li> <li>Benchmarks - See performance methodology and results</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions or share ideas</li> <li>Contributing: See contribution guidelines</li> <li>Enterprise: Explore enterprise features</li> </ul> <p>Built with  by BenevolentJoker-JohnL</p> <p>GitHub Sponsor</p>"},{"location":"enterprise/roadmap/","title":"Enterprise Roadmap","text":""},{"location":"enterprise/roadmap/#future-enterprise-opportunities","title":"Future Enterprise Opportunities","text":"<p>The free version includes everything you need for production deployments. These advanced features are available through custom development partnerships and sponsorship.</p>"},{"location":"enterprise/roadmap/#available-enterprise-features","title":"Available Enterprise Features","text":""},{"location":"enterprise/roadmap/#ray-train-integration","title":"\ud83d\udd27 Ray Train Integration","text":"<p>Distributed model fine-tuning across GPU clusters</p> <p>Train custom LLMs on your infrastructure with distributed training:</p> <ul> <li>Multi-node GPU orchestration</li> <li>Automatic checkpointing and recovery</li> <li>Distributed data loading</li> <li>Training metrics and visualization</li> <li>Integration with your existing models</li> </ul> <p>Use Case: Organizations training custom models on proprietary data</p>"},{"location":"enterprise/roadmap/#multi-region-orchestration","title":"\ud83c\udf10 Multi-Region Orchestration","text":"<p>Global load balancing with geo-aware routing</p> <p>Deploy SOLLOL across multiple regions with intelligent geo-routing:</p> <ul> <li>Latency-based routing (&lt;100ms worldwide)</li> <li>Regional failover and disaster recovery</li> <li>Data sovereignty compliance</li> <li>Cross-region traffic management</li> <li>Global health monitoring</li> </ul> <p>Use Case: Worldwide SaaS platforms requiring low-latency AI inference</p>"},{"location":"enterprise/roadmap/#advanced-analytics-suite","title":"\ud83d\udcca Advanced Analytics Suite","text":"<p>ML-powered capacity planning and cost optimization</p> <p>Predictive analytics for optimal resource utilization:</p> <ul> <li>ML-based demand forecasting</li> <li>Automated capacity recommendations</li> <li>Cost optimization algorithms</li> <li>Budget alerts and reporting</li> <li>Usage trend analysis</li> </ul> <p>Use Case: Enterprises optimizing cloud spend and resource allocation</p>"},{"location":"enterprise/roadmap/#enterprise-sso-integration","title":"\ud83d\udd10 Enterprise SSO Integration","text":"<p>SAML, OAuth2, LDAP, Active Directory</p> <p>Enterprise identity management integration:</p> <ul> <li>Single sign-on (SAML 2.0)</li> <li>OAuth2/OIDC providers</li> <li>LDAP/Active Directory sync</li> <li>Role mapping and provisioning</li> <li>Audit logging</li> </ul> <p>Use Case: Large organizations with existing identity infrastructure</p>"},{"location":"enterprise/roadmap/#custom-routing-engines","title":"\ud83c\udfaf Custom Routing Engines","text":"<p>Bespoke algorithms for specialized workloads</p> <p>Industry-specific routing optimizations:</p> <ul> <li>Custom scoring algorithms</li> <li>Domain-specific task detection</li> <li>Specialized performance metrics</li> <li>Workflow-aware routing</li> <li>Integration with internal systems</li> </ul> <p>Use Case: Companies with unique workload patterns or compliance requirements</p>"},{"location":"enterprise/roadmap/#sla-guarantees","title":"\ud83d\udee1\ufe0f SLA Guarantees","text":"<p>99.9%+ uptime with priority support</p> <p>Enterprise-grade reliability and support:</p> <ul> <li>99.9% uptime SLA</li> <li>4-hour incident response</li> <li>Dedicated support engineer</li> <li>Quarterly architecture reviews</li> <li>Custom runbooks and documentation</li> </ul> <p>Use Case: Mission-critical production systems requiring guaranteed uptime</p>"},{"location":"enterprise/roadmap/#dedicated-support","title":"\ud83d\udcde Dedicated Support","text":"<p>Hands-on partnership and architecture reviews</p> <p>Direct access to SOLLOL experts:</p> <ul> <li>Private Slack channel</li> <li>Weekly video calls</li> <li>Architecture design sessions</li> <li>Performance optimization reviews</li> <li>On-call engineering support</li> </ul> <p>Use Case: Teams needing expert guidance for complex deployments</p>"},{"location":"enterprise/roadmap/#custom-development","title":"\ud83c\udfd7\ufe0f Custom Development","text":"<p>Tailored features for your infrastructure</p> <p>Bespoke development services:</p> <ul> <li>Custom integrations (monitoring, logging, etc.)</li> <li>Proprietary protocol support</li> <li>Internal tool integration</li> <li>Feature development</li> <li>Migration assistance</li> </ul> <p>Use Case: Organizations with unique technical requirements</p>"},{"location":"enterprise/roadmap/#why-sponsorship","title":"Why Sponsorship?","text":"<p>Each enterprise feature involves:</p> <ul> <li>Months of development time - Complex features requiring significant engineering effort</li> <li>Enterprise integrations - Testing with SAML providers, LDAP servers, cloud platforms</li> <li>Ongoing maintenance - Security patches, compatibility updates, bug fixes</li> <li>Multi-environment testing - Validation across different infrastructure setups</li> <li>Comprehensive documentation - Architecture guides, deployment runbooks, training materials</li> </ul>"},{"location":"enterprise/roadmap/#get-started","title":"Get Started","text":""},{"location":"enterprise/roadmap/#engagement-process","title":"Engagement Process","text":"<ol> <li>Discovery Call - Discuss your requirements and use case</li> <li>Proposal - Detailed scope, timeline, and pricing</li> <li>Development - Fixed-price or retainer-based engagement</li> <li>Delivery - Feature implementation with documentation</li> <li>Support - Ongoing maintenance and updates</li> </ol>"},{"location":"enterprise/roadmap/#contact","title":"Contact","text":"<ul> <li>GitHub Sponsors: Sponsor SOLLOL Development</li> <li>Discussions: Start a conversation</li> <li>Email: Open a discussion for direct contact</li> </ul>"},{"location":"enterprise/roadmap/#pricing-models","title":"Pricing Models","text":"<p>Fixed-Price Projects - Defined scope and deliverables - Clear timeline and milestones - Typical range: $50k - $200k per feature</p> <p>Retainer Agreements - Monthly commitment for ongoing development - Flexible scope and priorities - Includes support and maintenance - Typical range: $20k - $50k/month</p> <p>Custom Packages - Combination of features - Volume discounts available - Long-term partnerships</p>"},{"location":"enterprise/roadmap/#existing-clients","title":"Existing Clients","text":"<p>Coming soon - enterprise client case studies and testimonials</p>"},{"location":"enterprise/roadmap/#faq","title":"FAQ","text":"<p>Q: Can I contribute enterprise features as open source?</p> <p>A: Absolutely! If you develop a feature and want to contribute it back, we welcome PRs. Enterprise features are those requiring sponsored development effort.</p> <p>Q: How long does custom development take?</p> <p>A: Typically 3-6 months per major feature, depending on complexity and integration requirements.</p> <p>Q: Do you offer proof-of-concept work?</p> <p>A: Yes, we can start with a small PoC (2-4 weeks) to validate feasibility before full development.</p> <p>Q: What happens if my needs change?</p> <p>A: Retainer agreements offer flexibility to adjust priorities. Fixed-price projects have change order processes.</p> <p>Ready to discuss enterprise features? Start a conversation \u2192</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get SOLLOL running in 5 minutes with Docker Compose.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>8GB+ RAM recommended</li> <li>(Optional) GPU for optimal performance</li> </ul>"},{"location":"getting-started/quick-start/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/BenevolentJoker-JohnL/SOLLOL.git\ncd SOLLOL\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-start-the-stack","title":"Step 2: Start the Stack","text":"<pre><code># Start SOLLOL + 3 Ollama nodes + Prometheus + Grafana\ndocker-compose up -d\n\n# Check status\ndocker-compose ps\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-pull-a-model","title":"Step 3: Pull a Model","text":"<pre><code># Pull llama3.2 on all nodes\ndocker exec sollol-ollama-node-1-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-2-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-3-1 ollama pull llama3.2\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-test-the-setup","title":"Step 4: Test the Setup","text":"<pre><code># Send a test request to SOLLOL (drop-in replacement on port 11434)\ncurl -X POST http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n\n# Or use the standard Ollama API (SOLLOL is transparent!)\nexport OLLAMA_HOST=localhost:11434\nollama run llama3.2 \"Hello!\"\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-view-the-dashboard","title":"Step 5: View the Dashboard","text":"<p>Open your browser:</p> <ul> <li>SOLLOL Dashboard: http://localhost:11434/dashboard.html</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Prometheus: http://localhost:9091</li> </ul>"},{"location":"getting-started/quick-start/#using-the-python-sdk","title":"Using the Python SDK","text":"<pre><code>from sollol import connect\n\n# Connect to SOLLOL (drop-in replacement - same port as Ollama!)\nsollol = connect(\"http://localhost:11434\")\n\n# Chat with intelligent routing\nresponse = sollol.chat(\n    \"Explain quantum computing\",\n    priority=8  # High priority = faster nodes\n)\n\nprint(response['message']['content'])\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize SOLLOL settings</li> <li>Architecture - Understand how it works</li> <li>Deployment - Deploy to production</li> <li>Benchmarks - Run performance tests</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#ollama-nodes-not-responding","title":"Ollama nodes not responding","text":"<pre><code># Check logs\ndocker-compose logs sollol\ndocker-compose logs ollama-node-1\n\n# Restart a node\ndocker-compose restart ollama-node-1\n</code></pre>"},{"location":"getting-started/quick-start/#port-conflicts","title":"Port conflicts","text":"<p>If port 11434 is already in use (e.g., you have Ollama running), stop it first:</p> <pre><code># Stop standalone Ollama (if running)\npkill ollama\n\n# Or change SOLLOL's port in docker-compose.yml\nports:\n  - \"11435:11434\"  # Change external port\n</code></pre>"},{"location":"getting-started/quick-start/#gpu-not-detected","title":"GPU not detected","text":"<p>Ensure NVIDIA Container Toolkit is installed:</p> <pre><code># Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre> <p>Then uncomment GPU sections in <code>docker-compose.yml</code>.</p>"},{"location":"getting-started/quick-start/#support","title":"Support","text":"<p>Need help? Open an issue on GitHub.</p>"}]}
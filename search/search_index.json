{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SOLLOL Documentation","text":"<p>Hybrid Cluster Orchestrator: Task Routing + Distributed Model Inference</p> <p>Quick Start View on GitHub</p>"},{"location":"#what-is-sollol","title":"What is SOLLOL?","text":"<p>SOLLOL is a hybrid cluster orchestrator that unifies task routing and distributed model inference for local LLM deployments.</p> <p>SOLLOL provides two complementary distribution strategies:</p>"},{"location":"#1-task-distribution-load-balancing","title":"1. Task Distribution (Load Balancing)","text":"<p>Intelligently routes multiple agent requests across Ollama nodes based on: - Task complexity (embedding vs generation vs analysis) - Resource availability (GPU memory, CPU load) - Real-time performance (latency, success rate) - Historical patterns (adaptive learning from past executions)</p>"},{"location":"#2-model-sharding-layer-level-distribution","title":"2. Model Sharding (Layer-Level Distribution)","text":"<p>Enables running models that don't fit on a single machine via llama.cpp RPC: - Layer distribution across multiple RPC backends - GGUF auto-extraction from Ollama blob storage - Verified with 13B models across 2-3 nodes - Trade-offs: Slower startup (2-5 min) and inference (~5 tok/s vs ~20 tok/s local)</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Intelligent Routing</p> <p>Context-aware analysis routes requests to optimal nodes using multi-factor scoring and adaptive learning.</p> <p> Learn more</p> </li> <li> <p> Priority Queue</p> <p>10-level priority system with age-based fairness ensures critical tasks get resources without starvation.</p> <p> Learn more</p> </li> <li> <p> Auto Failover</p> <p>3 retry attempts with exponential backoff and health monitoring ensure zero-downtime operation.</p> <p> Learn more</p> </li> <li> <p> Observability</p> <p>Real-time dashboard with Prometheus metrics and routing transparency for complete visibility.</p> <p> Learn more</p> </li> <li> <p> High Performance</p> <p>Ray actors and Dask batch processing deliver &lt;10ms routing overhead with 40-60% throughput gains.</p> <p> Learn more</p> </li> <li> <p> Enterprise Security</p> <p>SHA-256 API keys with RBAC permissions and per-key rate limiting for production deployments.</p> <p> Learn more</p> </li> </ul>"},{"location":"#performance-impact","title":"Performance Impact","text":"Metric Expected Improvement Avg Latency -30-40% (context-aware routing to optimal nodes) P95 Latency -40-50% (avoiding overloaded nodes) Success Rate +2-4pp (automatic failover and retry) Throughput +40-60% (better resource utilization) GPU Utilization +50-80% (intelligent task-to-GPU matching) <p>Production Ready</p> <p>SOLLOL is fully functional and production-ready. No artificial limits, no feature gates.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>Architecture Overview - Understand the system design</li> <li>Deployment Guide - Deploy with Docker or Kubernetes</li> <li>Benchmarks - See performance methodology and results</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Ask questions or share ideas</li> <li>Contributing: See contribution guidelines</li> <li>Enterprise: Explore enterprise features</li> </ul> <p>Built with  by BenevolentJoker-JohnL</p> <p>GitHub Sponsor</p>"},{"location":"MULTI_APP_OBSERVABILITY/","title":"Multi-App Observability with SOLLOL","text":""},{"location":"MULTI_APP_OBSERVABILITY/#overview","title":"Overview","text":"<p>SOLLOL v0.9.16+ supports multi-app observability, allowing multiple applications on the same machine to share a single unified dashboard instance. This provides centralized monitoring for all SOLLOL-powered applications without port conflicts or resource duplication.</p>"},{"location":"MULTI_APP_OBSERVABILITY/#how-it-works","title":"How It Works","text":""},{"location":"MULTI_APP_OBSERVABILITY/#automatic-fallback-mechanism","title":"Automatic Fallback Mechanism","text":"<p>When a SOLLOL application attempts to start the Unified Dashboard:</p> <ol> <li>Port Check: The dashboard checks if port 8080 (default) is already in use</li> <li>Fallback Detection: If occupied, assumes another SOLLOL dashboard is running</li> <li>Graceful Fallback: Logs connection info and continues without error</li> <li>Shared Observability: Both applications use the same dashboard for monitoring</li> </ol>"},{"location":"MULTI_APP_OBSERVABILITY/#key-features","title":"Key Features","text":"<ul> <li>\u2705 Zero Configuration: Automatic detection and fallback</li> <li>\u2705 No Port Conflicts: Multiple apps coexist peacefully</li> <li>\u2705 Centralized Monitoring: Single dashboard for all applications</li> <li>\u2705 Graceful Degradation: Apps continue running if dashboard unavailable</li> </ul>"},{"location":"MULTI_APP_OBSERVABILITY/#usage","title":"Usage","text":""},{"location":"MULTI_APP_OBSERVABILITY/#basic-example","title":"Basic Example","text":"<pre><code>from sollol import OllamaPool, UnifiedDashboard, RayHybridRouter\n\n# Create application infrastructure\npool = OllamaPool(nodes=[{\"host\": \"localhost\", \"port\": 11434}])\nrouter = RayHybridRouter(ollama_pool=pool, enable_distributed=True)\n\n# Create dashboard with fallback enabled (default)\ndashboard = UnifiedDashboard(router=router, dashboard_port=8080)\n\n# Start dashboard - automatically falls back if port occupied\ndashboard.run(allow_fallback=True)  # allow_fallback=True is default\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#expected-behavior","title":"Expected Behavior","text":"<p>First Application (starts dashboard): <pre><code>2025-10-07 09:00:00,000 - INFO - \ud83d\ude80 Starting Unified Dashboard on http://0.0.0.0:8080\n2025-10-07 09:00:00,100 - INFO - \u2705 Using Waitress production server\n</code></pre></p> <p>Second Application (detects existing dashboard): <pre><code>2025-10-07 09:01:00,000 - INFO - \ud83d\udcca Dashboard already running on port 8080\n2025-10-07 09:01:00,001 - INFO -    Connecting to existing dashboard at http://localhost:8080\n2025-10-07 09:01:00,002 - INFO - \u2705 Application will use shared dashboard for observability\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#configuration-options","title":"Configuration Options","text":""},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-initialization","title":"Dashboard Initialization","text":"<pre><code>dashboard = UnifiedDashboard(\n    router=router,\n    dashboard_port=8080,        # Dashboard port (default: 8080)\n    ray_dashboard_port=8265,    # Ray dashboard port (default: 8265)\n    dask_dashboard_port=8787,   # Dask dashboard port (default: 8787)\n)\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#run-method","title":"Run Method","text":"<pre><code>dashboard.run(\n    host=\"0.0.0.0\",            # Bind address (default: 0.0.0.0)\n    debug=False,               # Debug mode (default: False)\n    allow_fallback=True        # Enable fallback detection (default: True)\n)\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#disable-fallback-force-start","title":"Disable Fallback (Force Start)","text":"<p>If you want to force the dashboard to start and fail if the port is occupied:</p> <pre><code>dashboard.run(allow_fallback=False)\n</code></pre> <p>This will raise an <code>OSError</code> if the port is already in use.</p>"},{"location":"MULTI_APP_OBSERVABILITY/#architecture","title":"Architecture","text":""},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-components","title":"Dashboard Components","text":"<p>The Unified Dashboard provides monitoring for:</p> <ol> <li>Network Nodes: Ollama pool nodes with health status</li> <li>RPC Backends: llama.cpp RPC servers for model sharding</li> <li>Applications: Registered applications using SOLLOL</li> <li>Request Metrics: Real-time request/response stats</li> <li>Ray Dashboard: Distributed task monitoring (port 8265)</li> <li>Dask Dashboard: Batch processing monitoring (dynamic port)</li> </ol>"},{"location":"MULTI_APP_OBSERVABILITY/#multi-app-workflow","title":"Multi-App Workflow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Application 1  \u2502         \u2502  Application 2  \u2502\n\u2502  (SynapticLlamas\u2502         \u2502  (CustomApp)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                           \u2502\n         \u2502 1. Start dashboard        \u2502 2. Detect existing\n         \u2502    on port 8080           \u2502    dashboard on 8080\n         \u2502                           \u2502\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n         \u25bc               \u25bc           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Unified Dashboard (Port 8080)    \u2502\n    \u2502                                    \u2502\n    \u2502  \u2022 Network Nodes                   \u2502\n    \u2502  \u2022 RPC Backends                    \u2502\n    \u2502  \u2022 Applications: 2 registered      \u2502\n    \u2502  \u2022 Request Metrics                 \u2502\n    \u2502  \u2022 Ray Dashboard \u2192 :8265           \u2502\n    \u2502  \u2022 Dask Dashboard \u2192 :auto          \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#real-world-example","title":"Real-World Example","text":""},{"location":"MULTI_APP_OBSERVABILITY/#synapticllamas-integration","title":"SynapticLlamas Integration","text":"<pre><code># In SynapticLlamas main.py\nfrom sollol import UnifiedDashboard, RayHybridRouter\nfrom sollol.dashboard_client import DashboardClient\n\n# Create distributed router\nrouter = RayHybridRouter(\n    ollama_pool=ollama_pool,\n    rpc_backends=rpc_backends,\n    enable_distributed=True\n)\n\n# Register application with dashboard\nclient = DashboardClient(\n    app_name=\"SynapticLlamas\",\n    router_type=\"RayHybridRouter\",\n    version=\"1.0.0\",\n    dashboard_url=\"http://localhost:8080\",\n    auto_register=True\n)\n\n# In dashboard command handler\nif command == 'dashboard':\n    dashboard = UnifiedDashboard(\n        router=router,\n        dashboard_port=8080\n    )\n    dashboard.run(allow_fallback=True)  # Graceful fallback\n</code></pre> <p>If another SOLLOL app (like a custom inference service) is already running with a dashboard, SynapticLlamas will detect it and share the same dashboard.</p>"},{"location":"MULTI_APP_OBSERVABILITY/#testing","title":"Testing","text":""},{"location":"MULTI_APP_OBSERVABILITY/#test-script","title":"Test Script","text":"<pre><code># Start first app (SynapticLlamas)\ncd ~/SynapticLlamas\npython3 main.py --distributed\n# Type: dashboard\n\n# In another terminal, test fallback\ncd ~/SOLLOL\npython3 test_dashboard_fallback_simple.py\n</code></pre> <p>Expected output: <pre><code>\u2705 Dashboard is already running on port 8080\nTesting fallback behavior:\nAttempting to start dashboard on port 8080 (already occupied)...\n\ud83d\udcca Dashboard already running on port 8080\n   Connecting to existing dashboard at http://localhost:8080\n\u2705 Application will use shared dashboard for observability\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-not-accessible","title":"Dashboard Not Accessible","text":"<p>Issue: Dashboard fallback detected but cannot access http://localhost:8080</p> <p>Solution: Check if the first application's dashboard is actually running: <pre><code>curl http://localhost:8080/api/health\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#port-conflicts","title":"Port Conflicts","text":"<p>Issue: Want to run dashboards on different ports for different apps</p> <p>Solution: Use custom ports for each application: <pre><code># App 1\ndashboard1 = UnifiedDashboard(router=router1, dashboard_port=8080)\n\n# App 2\ndashboard2 = UnifiedDashboard(router=router2, dashboard_port=8081)\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#disable-fallback","title":"Disable Fallback","text":"<p>Issue: Need to ensure dashboard starts fresh each time</p> <p>Solution: Disable fallback and handle errors manually: <pre><code>try:\n    dashboard.run(allow_fallback=False)\nexcept OSError as e:\n    if \"Address already in use\" in str(e):\n        # Kill existing process and retry\n        subprocess.run([\"pkill\", \"-f\", \"waitress\"])\n        dashboard.run(allow_fallback=False)\n    else:\n        raise\n</code></pre></p>"},{"location":"MULTI_APP_OBSERVABILITY/#best-practices","title":"Best Practices","text":"<ol> <li>Enable Fallback by Default: Use <code>allow_fallback=True</code> for production apps</li> <li>Single Dashboard Per Machine: Let one \"primary\" app run the dashboard</li> <li>Dashboard Client Registration: Register all apps with <code>DashboardClient</code> for visibility</li> <li>Health Checks: Monitor <code>/api/health</code> endpoint for dashboard availability</li> <li>Graceful Shutdown: Ensure dashboard cleanup on app exit</li> </ol>"},{"location":"MULTI_APP_OBSERVABILITY/#api-reference","title":"API Reference","text":""},{"location":"MULTI_APP_OBSERVABILITY/#unifieddashboardrun","title":"UnifiedDashboard.run()","text":"<pre><code>def run(self, host: str = \"0.0.0.0\", debug: bool = False, allow_fallback: bool = True) -&gt; None:\n    \"\"\"\n    Run dashboard server (production-ready with Waitress).\n\n    Args:\n        host: Bind address (default: 0.0.0.0)\n        debug: Enable Flask debug mode (default: False)\n        allow_fallback: If True and port is in use, assume another dashboard is running (default: True)\n\n    Raises:\n        OSError: If port is in use and allow_fallback=False\n    \"\"\"\n</code></pre>"},{"location":"MULTI_APP_OBSERVABILITY/#dashboard-endpoints","title":"Dashboard Endpoints","text":"<ul> <li><code>GET /</code> - Dashboard UI</li> <li><code>GET /api/health</code> - Health check</li> <li><code>GET /api/metrics</code> - Current metrics</li> <li><code>GET /api/dashboard/config</code> - Dashboard configuration (Ray/Dask ports)</li> <li><code>GET /api/applications</code> - Registered applications</li> <li><code>GET /api/nodes</code> - Ollama pool nodes</li> <li><code>GET /api/rpc_backends</code> - RPC backend status</li> <li><code>WS /events</code> - Real-time event stream</li> </ul>"},{"location":"MULTI_APP_OBSERVABILITY/#version-history","title":"Version History","text":"<ul> <li>v0.9.16: Added multi-app fallback with <code>allow_fallback</code> parameter</li> <li>v0.9.15: Added SOLLOL version logging</li> <li>v0.9.14: Ray OOM prevention</li> <li>v0.9.13: Fixed async/dict errors in metrics endpoint</li> <li>v0.9.12: Optimized panel sizing</li> <li>v0.9.7: Added dynamic port detection for Dask</li> </ul>"},{"location":"MULTI_APP_OBSERVABILITY/#see-also","title":"See Also","text":"<ul> <li>Unified Dashboard Documentation</li> <li>Ray Integration Guide</li> <li>Dask Integration Guide</li> <li>Application Registration</li> </ul>"},{"location":"layer_partitioning/","title":"Layer Partitioning for Large Models","text":"<p>SOLLOL now supports layer partitioning - the ability to split large models (70B+) across multiple nodes for distributed inference.</p>"},{"location":"layer_partitioning/#overview","title":"Overview","text":"<p>What it does: - Splits models too large for a single GPU across multiple nodes - Each node loads specific layers (e.g., node1: layers 0-39, node2: layers 40-79) - Coordinates inference across nodes automatically - Provides both vertical scaling (bigger models) and horizontal scaling (more throughput)</p> <p>When to use it: - Models larger than your single-node GPU memory (Llama-70B, Mixtral-8x7B, etc.) - You have multiple GPUs across different machines - You want to run models that wouldn't fit otherwise</p>"},{"location":"layer_partitioning/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    SOLLOL Gateway                    \u2502\n\u2502                                                      \u2502\n\u2502  Request for llama2:70b                              \u2502\n\u2502    \u2193                                                 \u2502\n\u2502  Routing Decision:                                   \u2502\n\u2502    - Small model (llama3.2) \u2192 Individual node        \u2502\n\u2502    - Large model (llama2:70b) \u2192 Cluster              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                             \u2502\n        \u25bc                             \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Individual     \u2502         \u2502   Node Cluster   \u2502\n\u2502      Node        \u2502         \u2502   (llama2:70b)   \u2502\n\u2502                  \u2502         \u2502                  \u2502\n\u2502  \u2022 llama3.2      \u2502         \u2502  Node 1:         \u2502\n\u2502  \u2022 phi           \u2502         \u2502    Layers 0-39   \u2502\n\u2502  \u2022 codellama     \u2502         \u2502                  \u2502\n\u2502                  \u2502         \u2502  Node 2:         \u2502\n\u2502  Full model      \u2502         \u2502    Layers 40-79  \u2502\n\u2502  on single GPU   \u2502         \u2502                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  Distributed     \u2502\n                             \u2502  inference       \u2502\n                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"layer_partitioning/#quick-start","title":"Quick Start","text":""},{"location":"layer_partitioning/#1-add-nodes-to-registry","title":"1. Add Nodes to Registry","text":"<pre><code>from sollol.registry import NodeRegistry\n\nregistry = NodeRegistry()\n\n# Add individual nodes\nregistry.add_node(\"http://192.168.1.10:11434\", name=\"gpu-node-1\")\nregistry.add_node(\"http://192.168.1.11:11434\", name=\"gpu-node-2\")\nregistry.add_node(\"http://192.168.1.12:11434\", name=\"gpu-node-3\")\n</code></pre>"},{"location":"layer_partitioning/#2-create-cluster-for-large-model","title":"2. Create Cluster for Large Model","text":"<pre><code># Create cluster for Llama-70B across 2 nodes\ncluster = registry.create_cluster(\n    name=\"llama70b-cluster\",\n    node_urls=[\n        \"http://192.168.1.10:11434\",\n        \"http://192.168.1.11:11434\"\n    ],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"even\"  # or \"memory_aware\"\n)\n\n# Output:\n# \ud83d\udce6 Created cluster 'llama70b-cluster' with 2 nodes for llama2:70b\n#    Node 1: layers 0-39 (40 layers)\n#    Node 2: layers 40-79 (40 layers)\n</code></pre>"},{"location":"layer_partitioning/#3-use-smart-routing","title":"3. Use Smart Routing","text":"<pre><code># Get best worker for model (automatically selects cluster for large models)\nworker = registry.get_worker_for_model(\"llama2:70b\")\n\nif isinstance(worker, NodeCluster):\n    print(f\"Using cluster: {worker.name}\")\n    result = await worker.generate(\"Explain quantum computing\")\nelse:\n    print(f\"Using single node: {worker.name}\")\n</code></pre>"},{"location":"layer_partitioning/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"layer_partitioning/#even-distribution-default","title":"Even Distribution (Default)","text":"<p>Splits layers evenly across nodes:</p> <pre><code>cluster = registry.create_cluster(\n    name=\"my-cluster\",\n    node_urls=[...],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"even\"\n)\n\n# 80 layers across 2 nodes:\n# Node 1: 40 layers (0-39)\n# Node 2: 40 layers (40-79)\n</code></pre>"},{"location":"layer_partitioning/#memory-aware-distribution","title":"Memory-Aware Distribution","text":"<p>Allocates layers proportionally to available memory:</p> <pre><code>cluster = registry.create_cluster(\n    name=\"my-cluster\",\n    node_urls=[...],\n    model=\"llama2:70b\",\n    partitioning_strategy=\"memory_aware\"\n)\n\n# If Node 1 has 48GB and Node 2 has 24GB:\n# Node 1: 53 layers (0-52)  # 66% of layers\n# Node 2: 27 layers (53-79)  # 33% of layers\n</code></pre>"},{"location":"layer_partitioning/#supported-models","title":"Supported Models","text":""},{"location":"layer_partitioning/#large-models-require-partitioning","title":"Large Models (Require Partitioning)","text":"<ul> <li>llama2:70b - 80 layers, ~36GB memory</li> <li>llama3:70b - 80 layers, ~36GB memory</li> <li>mixtral:8x7b - 32 layers (MoE), ~26GB memory</li> </ul>"},{"location":"layer_partitioning/#small-models-single-node","title":"Small Models (Single Node)","text":"<ul> <li>llama3.2 - 32 layers, ~2GB memory</li> <li>phi - 32 layers, ~1.5GB memory</li> <li>codellama:7b - 32 layers, ~4GB memory</li> </ul>"},{"location":"layer_partitioning/#adding-custom-models","title":"Adding Custom Models","text":"<p>Edit <code>sollol/node_cluster.py</code> to add model specs:</p> <pre><code>MODEL_SPECS = {\n    \"your-model:70b\": ModelSpec(\n        name=\"your-model:70b\",\n        total_layers=80,\n        memory_per_layer_mb=450,\n        min_memory_mb=4096\n    ),\n}\n</code></pre>"},{"location":"layer_partitioning/#health-checking","title":"Health Checking","text":"<p>Clusters require ALL nodes to be healthy:</p> <pre><code># Check cluster health\nis_healthy = await cluster.health_check()\n\nif not is_healthy:\n    print(f\"Cluster unhealthy - nodes down: {[n.url for n in cluster.nodes if not n.is_healthy]}\")\n\n# Check all clusters\ncluster_health = await registry.health_check_clusters()\n</code></pre>"},{"location":"layer_partitioning/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom sollol.registry import NodeRegistry\n\nasync def main():\n    # Setup registry\n    registry = NodeRegistry()\n\n    # Discover nodes on network\n    registry.discover_nodes(cidr=\"192.168.1.0/24\")\n\n    # Create cluster for large model\n    if len(registry.get_healthy_nodes()) &gt;= 2:\n        cluster = registry.create_cluster(\n            name=\"llama70b\",\n            node_urls=[\n                registry.get_healthy_nodes()[0].url,\n                registry.get_healthy_nodes()[1].url\n            ],\n            model=\"llama2:70b\"\n        )\n\n        # Run inference\n        result = await cluster.generate(\n            prompt=\"Write a detailed explanation of quantum entanglement\",\n            options={\"temperature\": 0.7}\n        )\n\n        print(result['response'])\n        print(f\"\\nCluster info: {result['_cluster']}\")\n\n    # Small models use individual nodes\n    worker = registry.get_worker_for_model(\"llama3.2\")\n    print(f\"Small model routed to: {worker.name}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"layer_partitioning/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"layer_partitioning/#latency-trade-offs","title":"Latency Trade-offs","text":"<ul> <li>Single Node: Fastest (no inter-node communication)</li> <li>Cluster (2 nodes): ~10-20% slower due to coordination overhead</li> <li>Cluster (3+ nodes): Additional latency per node</li> </ul>"},{"location":"layer_partitioning/#throughput-benefits","title":"Throughput Benefits","text":"<ul> <li>Load Balancing: Small models spread across available nodes</li> <li>Large Model Access: Run models impossible on single GPU</li> <li>Mixed Workloads: Clusters handle 70B while other nodes serve 7B/13B</li> </ul>"},{"location":"layer_partitioning/#optimal-configurations","title":"Optimal Configurations","text":"<p>2-Node Cluster (Recommended for 70B models) <pre><code>GPU 1: 24GB - Layers 0-39 of Llama-70B\nGPU 2: 24GB - Layers 40-79 of Llama-70B\n</code></pre></p> <p>3-Node Cluster (For extremely large or multiple 70B models) <pre><code>GPU 1: 24GB - Layers 0-26\nGPU 2: 24GB - Layers 27-53\nGPU 3: 24GB - Layers 54-79\n</code></pre></p>"},{"location":"layer_partitioning/#limitations","title":"Limitations","text":"<p>Current implementation: - \u2705 Layer partitioning logic and cluster management - \u2705 Health checking and failover - \u2705 Smart routing (large \u2192 cluster, small \u2192 single) - \u26a0\ufe0f  Inter-node communication (basic implementation) - \u26a0\ufe0f  Requires Ollama layer partitioning support (WIP upstream)</p> <p>Future enhancements: 1. gRPC for faster inter-node communication 2. Session affinity for multi-turn conversations 3. Dynamic layer rebalancing based on load 4. Automatic cluster creation on demand</p>"},{"location":"layer_partitioning/#comparison-sollol-vs-ollol","title":"Comparison: SOLLOL vs OLLOL","text":"Feature SOLLOL OLLOL (K2/olol) Load balancing \u2705 Advanced \u2705 Basic Layer partitioning \u2705 New \u2705 Existing Health scoring \u2705 Performance-based \u2705 Simple ping Auto-discovery \u2705 CIDR scanning \u2705 Broadcast Intelligent routing \u2705 Task-aware \u274c Priority queuing \u2705 \u274c Observability \u2705 Dashboard \u274c <p>SOLLOL now provides both capabilities from OLLOL: - Load balancing across independent workers (existing) - Layer partitioning for large models (new)</p>"},{"location":"layer_partitioning/#see-also","title":"See Also","text":"<ul> <li>Node Registry Documentation</li> <li>Intelligent Routing</li> <li>Network Discovery</li> </ul>"},{"location":"llama_cpp_guide/","title":"llama.cpp Model Sharding Guide","text":"<p>Complete guide to running large language models across multiple machines using SOLLOL's llama.cpp integration.</p>"},{"location":"llama_cpp_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>What is Model Sharding?</li> <li>Architecture</li> <li>When to Use Model Sharding</li> <li>Setup Guide</li> <li>Usage Examples</li> <li>Model Profiles</li> <li>Performance &amp; Optimization</li> <li>Troubleshooting</li> <li>Advanced Topics</li> </ol>"},{"location":"llama_cpp_guide/#overview","title":"Overview","text":"<p>SOLLOL integrates with llama.cpp to enable model sharding - the ability to run models that are too large to fit on a single GPU by distributing them across multiple machines.</p>"},{"location":"llama_cpp_guide/#key-benefits","title":"Key Benefits","text":"<ul> <li>\u2705 Run 70B+ models on machines with limited VRAM</li> <li>\u2705 Automatic GGUF extraction from Ollama storage</li> <li>\u2705 Zero-config setup with auto-discovery</li> <li>\u2705 Seamless integration with SOLLOL's intelligent routing</li> <li>\u2705 Hybrid operation - small models use Ollama, large models use sharding</li> </ul>"},{"location":"llama_cpp_guide/#what-you-get","title":"What You Get","text":"<pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\n# Auto-configure with model sharding enabled\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    num_rpc_backends=3  # Shard across 3 machines\n)\n\n# Small models \u2192 Ollama (fast, local)\nresponse = router.route_request(\n    model=\"llama3.2\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Large models \u2192 llama.cpp sharding (distributed)\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[{\"role\": \"user\", \"content\": \"Complex task...\"}]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#what-is-model-sharding","title":"What is Model Sharding?","text":""},{"location":"llama_cpp_guide/#the-problem","title":"The Problem","text":"<p>Large language models like Llama 3.1 70B require ~40GB of VRAM. If you only have GPUs with 24GB VRAM, you can't run these models locally.</p> <p>Traditional options: - \u274c Cloud APIs (expensive, privacy concerns) - \u274c Upgrade to more expensive hardware - \u274c Use smaller, less capable models</p>"},{"location":"llama_cpp_guide/#the-solution-model-sharding","title":"The Solution: Model Sharding","text":"<p>Model sharding distributes a single model across multiple machines:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Llama 3.1 70B Model (40GB total)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502              \u2502              \u2502\n        \u25bc              \u25bc              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Machine 1  \u2502 \u2502   Machine 2  \u2502 \u2502   Machine 3  \u2502\n\u2502              \u2502 \u2502              \u2502 \u2502              \u2502\n\u2502 Layers 0-26  \u2502 \u2502 Layers 27-53 \u2502 \u2502 Layers 54-79 \u2502\n\u2502   (~13GB)    \u2502 \u2502   (~13GB)    \u2502 \u2502   (~13GB)    \u2502\n\u2502              \u2502 \u2502              \u2502 \u2502              \u2502\n\u2502 RTX 4090     \u2502 \u2502 RTX 4090     \u2502 \u2502 RTX 4090     \u2502\n\u2502  24GB VRAM   \u2502 \u2502  24GB VRAM   \u2502 \u2502  24GB VRAM   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>How it works: 1. Model layers are split across machines 2. During inference, data flows through each machine sequentially 3. llama.cpp RPC (Remote Procedure Call) handles communication 4. SOLLOL coordinates everything automatically</p>"},{"location":"llama_cpp_guide/#architecture","title":"Architecture","text":""},{"location":"llama_cpp_guide/#components","title":"Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      SOLLOL Gateway                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              HybridRouter                            \u2502  \u2502\n\u2502  \u2502  \u2022 Analyzes model requirements                       \u2502  \u2502\n\u2502  \u2502  \u2022 Routes small models \u2192 Ollama                      \u2502  \u2502\n\u2502  \u2502  \u2502  \u2022 Routes large models \u2192 llama.cpp coordinator    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502                         \u2502\n     \u25bc                         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Ollama    \u2502       \u2502   llama.cpp Coordinator              \u2502\n\u2502   Nodes     \u2502       \u2502   (llama-server)                     \u2502\n\u2502             \u2502       \u2502                                      \u2502\n\u2502 \u2022 llama3.2  \u2502       \u2502   \u2022 Loads GGUF model                 \u2502\n\u2502 \u2022 phi       \u2502       \u2502   \u2022 Distributes layers to RPC nodes  \u2502\n\u2502 \u2022 codellama \u2502       \u2502   \u2022 Coordinates inference            \u2502\n\u2502             \u2502       \u2502   \u2022 Returns results to SOLLOL        \u2502\n\u2502  (Fast,     \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502   local)    \u2502                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502                     \u2502              \u2502\n                        \u25bc                     \u25bc              \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 RPC Node \u2502          \u2502 RPC Node \u2502  \u2502 RPC Node \u2502\n                  \u2502    #1    \u2502          \u2502    #2    \u2502  \u2502    #3    \u2502\n                  \u2502          \u2502          \u2502          \u2502  \u2502          \u2502\n                  \u2502 Layers   \u2502          \u2502 Layers   \u2502  \u2502 Layers   \u2502\n                  \u2502  0-26    \u2502          \u2502  27-53   \u2502  \u2502  54-79   \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"llama_cpp_guide/#key-components-explained","title":"Key Components Explained","text":"<p>1. HybridRouter - Analyzes incoming requests - Determines if model needs sharding - Routes to appropriate backend</p> <p>2. llama.cpp Coordinator (llama-server) - Central control process - Loads the GGUF model file - Distributes layers to RPC backends - Coordinates inference passes</p> <p>3. RPC Backends (rpc-server) - Worker processes on each machine - Execute inference for assigned layers - Communicate via gRPC</p> <p>4. GGUF Extraction - SOLLOL automatically finds GGUFs in Ollama storage - No manual file management needed</p>"},{"location":"llama_cpp_guide/#when-to-use-model-sharding","title":"When to Use Model Sharding","text":""},{"location":"llama_cpp_guide/#use-model-sharding-when","title":"Use Model Sharding When:","text":"<p>\u2705 Model is too large for single GPU - Llama 3.1 70B (~40GB) on 24GB GPUs - Mixtral 8x7B (~26GB) on 16GB GPUs - Any model &gt; available VRAM</p> <p>\u2705 You have multiple machines with GPUs - 2-4 machines with GPUs - Network connection between them - Want to utilize distributed resources</p> <p>\u2705 Throughput is acceptable - Understand ~2-5x slower than local inference - Startup time (2-5 minutes) is acceptable - Network latency is reasonable (&lt;10ms)</p>"},{"location":"llama_cpp_guide/#dont-use-model-sharding-when","title":"Don't Use Model Sharding When:","text":"<p>\u274c Model fits on single GPU - Use Ollama directly (much faster) - Example: Llama 3.2 3B, Phi-3, CodeLlama 7B</p> <p>\u274c Need lowest latency - Model sharding adds network overhead - Better: Use smaller model or upgrade hardware</p> <p>\u274c Poor network connectivity - High latency (&gt;50ms) kills performance - RPC requires fast, reliable network</p>"},{"location":"llama_cpp_guide/#setup-guide","title":"Setup Guide","text":""},{"location":"llama_cpp_guide/#prerequisites","title":"Prerequisites","text":"<p>Hardware: - 2+ machines with GPUs (or CPUs for testing) - Network connectivity between machines - Sufficient VRAM across machines for model</p> <p>Software: - Python 3.8+ - Ollama installed (for GGUF extraction) - CMake (for building llama.cpp) - Git</p>"},{"location":"llama_cpp_guide/#option-1-auto-setup-recommended","title":"Option 1: Auto-Setup (Recommended)","text":"<p>SOLLOL can automatically setup llama.cpp RPC backends:</p> <pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\n# Auto-setup everything\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    auto_discover_rpc=True,    # Try to find existing RPC servers\n    auto_setup_rpc=True,        # Build/start RPC if not found\n    num_rpc_backends=3          # Number of RPC servers to start\n)\n\n# SOLLOL will:\n# 1. Look for running RPC servers on the network\n# 2. If not found, clone llama.cpp repository\n# 3. Build llama.cpp with RPC support\n# 4. Start RPC servers on available ports\n# 5. Configure HybridRouter to use them\n</code></pre> <p>What auto-setup does: 1. Checks for <code>llama.cpp</code> directory in <code>~/llama.cpp</code> 2. If not found, clones from GitHub 3. Builds with <code>cmake -DGGML_RPC=ON</code> 4. Starts <code>rpc-server</code> processes on ports 50052, 50053, etc. 5. Configures coordinator to use these backends</p>"},{"location":"llama_cpp_guide/#option-2-manual-setup","title":"Option 2: Manual Setup","text":"<p>For more control, setup llama.cpp manually:</p> <p>Step 1: Install llama.cpp</p> <pre><code># Clone llama.cpp\ncd ~\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\n\n# Build with RPC support\ncmake -B build -DGGML_RPC=ON -DLLAMA_CURL=OFF\ncmake --build build --config Release -j$(nproc)\n</code></pre> <p>Step 2: Start RPC Servers</p> <p>On each machine that will participate in sharding:</p> <pre><code># Machine 1\n~/llama.cpp/build/bin/rpc-server -H 0.0.0.0 -p 50052\n\n# Machine 2\n~/llama.cpp/build/bin/rpc-server -H 0.0.0.0 -p 50052\n\n# Machine 3\n~/llama.cpp/build/bin/rpc-server -H 0.0.0.0 -p 50052\n</code></pre> <p>Step 3: Configure SOLLOL</p> <pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\n# Manual RPC backend configuration\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    rpc_backends=[\n        {\"host\": \"192.168.1.10\", \"port\": 50052},\n        {\"host\": \"192.168.1.11\", \"port\": 50052},\n        {\"host\": \"192.168.1.12\", \"port\": 50052},\n    ]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#option-3-using-environment-variables","title":"Option 3: Using Environment Variables","text":"<pre><code># Set RPC backends via environment\nexport RPC_BACKENDS=\"192.168.1.10:50052,192.168.1.11:50052,192.168.1.12:50052\"\n\n# Run SOLLOL gateway\npython -m sollol.gateway\n</code></pre> <pre><code># HybridRouter will pick up RPC_BACKENDS automatically\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True\n)\n</code></pre>"},{"location":"llama_cpp_guide/#verification","title":"Verification","text":"<p>Check that RPC backends are accessible:</p> <pre><code># Test RPC connectivity\nnc -zv 192.168.1.10 50052\nnc -zv 192.168.1.11 50052\nnc -zv 192.168.1.12 50052\n</code></pre> <pre><code># Verify in Python\nfrom sollol.rpc_discovery import test_rpc_backend\n\nresult = test_rpc_backend(\"192.168.1.10\", 50052)\nprint(f\"RPC backend: {'\u2713 Available' if result else '\u2717 Not available'}\")\n</code></pre>"},{"location":"llama_cpp_guide/#usage-examples","title":"Usage Examples","text":""},{"location":"llama_cpp_guide/#example-1-basic-model-sharding","title":"Example 1: Basic Model Sharding","text":"<pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\nfrom sollol.priority_helpers import Priority\n\n# Setup router with model sharding\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    num_rpc_backends=3\n)\n\n# Small model - uses Ollama (fast)\nprint(\"Running small model...\")\nresponse = router.route_request(\n    model=\"llama3.2\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    priority=Priority.HIGH\n)\nprint(f\"Backend: {response.get('_routing', {}).get('backend')}\")\n# Output: Backend: ollama-pool\n\n# Large model - uses llama.cpp sharding (distributed)\nprint(\"\\nRunning large model...\")\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[{\"role\": \"user\", \"content\": \"Explain quantum computing\"}],\n    priority=Priority.NORMAL\n)\nprint(f\"Backend: {response.get('_routing', {}).get('backend')}\")\n# Output: Backend: llama.cpp-distributed\n</code></pre>"},{"location":"llama_cpp_guide/#example-2-check-model-routing-decision","title":"Example 2: Check Model Routing Decision","text":"<pre><code># Check which backend will be used before making request\nmodel = \"llama3.1:70b\"\nwill_use_sharding = router.should_use_distributed(model)\n\nif will_use_sharding:\n    print(f\"{model} will use distributed inference (llama.cpp)\")\n    print(\"Expected: Slower startup, network overhead\")\nelse:\n    print(f\"{model} will use local Ollama\")\n    print(\"Expected: Fast, low latency\")\n</code></pre>"},{"location":"llama_cpp_guide/#example-3-monitor-coordinator-status","title":"Example 3: Monitor Coordinator Status","text":"<pre><code># Get coordinator information\nif router.coordinator:\n    print(f\"Coordinator running: {router.coordinator.is_running()}\")\n    print(f\"Coordinator model: {router.coordinator_model}\")\n    print(f\"RPC backends: {len(router.coordinator.rpc_backends)}\")\n    print(f\"Coordinator URL: {router.coordinator.base_url}\")\nelse:\n    print(\"No coordinator active (using Ollama only)\")\n</code></pre>"},{"location":"llama_cpp_guide/#example-4-async-usage","title":"Example 4: Async Usage","text":"<pre><code>import asyncio\nfrom sollol import HybridRouter, OllamaPool\n\nasync def run_distributed_inference():\n    # Create router (async version)\n    pool = await OllamaPool.auto_configure()\n    router = HybridRouter(\n        ollama_pool=pool,\n        enable_distributed=True,\n        num_rpc_backends=3\n    )\n\n    # Run inference\n    response = await router.route_request(\n        model=\"llama3.1:70b\",\n        messages=[{\"role\": \"user\", \"content\": \"What is AGI?\"}]\n    )\n\n    print(response['message']['content'])\n\nasyncio.run(run_distributed_inference())\n</code></pre>"},{"location":"llama_cpp_guide/#example-5-multi-agent-with-mixed-models","title":"Example 5: Multi-Agent with Mixed Models","text":"<pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\nfrom sollol.priority_helpers import get_priority_for_role\n\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    num_rpc_backends=3\n)\n\nagents = [\n    {\"name\": \"Researcher\", \"role\": \"researcher\", \"model\": \"llama3.1:70b\"},  # Sharded\n    {\"name\": \"Editor\", \"role\": \"editor\", \"model\": \"llama3.2\"},              # Local\n    {\"name\": \"Summarizer\", \"role\": \"summarizer\", \"model\": \"llama3.2\"},      # Local\n]\n\nfor agent in agents:\n    priority = get_priority_for_role(agent[\"role\"])\n\n    response = router.route_request(\n        model=agent[\"model\"],\n        messages=[{\"role\": \"user\", \"content\": f\"Task for {agent['name']}\"}],\n        priority=priority\n    )\n\n    backend = response.get('_routing', {}).get('backend', 'unknown')\n    print(f\"{agent['name']} ({agent['model']}): {backend}\")\n</code></pre>"},{"location":"llama_cpp_guide/#model-profiles","title":"Model Profiles","text":"<p>SOLLOL uses model profiles to automatically determine routing strategy:</p>"},{"location":"llama_cpp_guide/#built-in-profiles","title":"Built-in Profiles","text":"<pre><code>MODEL_PROFILES = {\n    # Small models - Ollama\n    \"llama3.2\": {\n        \"parameter_count\": 3,\n        \"estimated_memory_gb\": 2,\n        \"requires_distributed\": False\n    },\n    \"phi\": {\n        \"parameter_count\": 3,\n        \"estimated_memory_gb\": 1.5,\n        \"requires_distributed\": False\n    },\n\n    # Medium models - Ollama (if fits)\n    \"llama3.1:8b\": {\n        \"parameter_count\": 8,\n        \"estimated_memory_gb\": 5,\n        \"requires_distributed\": False\n    },\n    \"codellama:13b\": {\n        \"parameter_count\": 13,\n        \"estimated_memory_gb\": 8,\n        \"requires_distributed\": False\n    },\n\n    # Large models - llama.cpp sharding\n    \"llama3.1:70b\": {\n        \"parameter_count\": 70,\n        \"estimated_memory_gb\": 40,\n        \"requires_distributed\": True\n    },\n    \"llama3.1:405b\": {\n        \"parameter_count\": 405,\n        \"estimated_memory_gb\": 240,\n        \"requires_distributed\": True\n    },\n    \"mixtral:8x7b\": {\n        \"parameter_count\": 47,  # MoE model\n        \"estimated_memory_gb\": 26,\n        \"requires_distributed\": True\n    }\n}\n</code></pre>"},{"location":"llama_cpp_guide/#custom-model-profiles","title":"Custom Model Profiles","text":"<p>Add your own model profiles:</p> <pre><code>from sollol.hybrid_router import MODEL_PROFILES\n\n# Add custom model\nMODEL_PROFILES[\"custom-70b\"] = {\n    \"parameter_count\": 70,\n    \"estimated_memory_gb\": 42,\n    \"requires_distributed\": True\n}\n\n# Now SOLLOL will route it to llama.cpp automatically\nrouter.route_request(\n    model=\"custom-70b\",\n    messages=[...]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#threshold-configuration","title":"Threshold Configuration","text":"<p>Adjust when sharding is used:</p> <pre><code>router = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    distributed_threshold_params=30,  # Shard models &gt; 30B parameters\n    num_rpc_backends=3\n)\n</code></pre>"},{"location":"llama_cpp_guide/#performance-optimization","title":"Performance &amp; Optimization","text":""},{"location":"llama_cpp_guide/#performance-characteristics","title":"Performance Characteristics","text":"<p>Startup Time: - First request: 2-5 minutes (model loading + layer distribution) - Subsequent requests: &lt;1 second (coordinator reuse)</p> <p>Inference Speed: - Local Ollama: ~20-40 tokens/sec (single GPU) - 2-node sharding: ~5-10 tokens/sec (~3-4\u00d7 slower) - 3-node sharding: ~3-7 tokens/sec (~5-6\u00d7 slower)</p> <p>Network Impact: <pre><code>Latency Impact:\n- &lt;1ms: Excellent (local network)\n- 1-10ms: Good (same datacenter)\n- 10-50ms: Acceptable (same region)\n- &gt;50ms: Poor (cross-region)\n</code></pre></p>"},{"location":"llama_cpp_guide/#optimization-tips","title":"Optimization Tips","text":"<p>1. Minimize RPC Hops <pre><code># Good: 2-3 backends (fewer network hops)\nrouter = HybridRouter(num_rpc_backends=2)\n\n# Avoid: 5+ backends (too many hops)\nrouter = HybridRouter(num_rpc_backends=6)\n</code></pre></p> <p>2. Use Fast Network <pre><code># Check network latency between machines\nping -c 10 192.168.1.11\n\n# Ensure &lt;10ms latency for good performance\n</code></pre></p> <p>3. Optimize Context Size <pre><code># Smaller context = faster inference\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[...],\n    max_tokens=512  # Limit response length\n)\n</code></pre></p> <p>4. Coordinator Reuse <pre><code># Coordinator stays loaded between requests\n# Subsequent requests are much faster\n\n# First request: 2-5 min (startup + inference)\nresponse1 = router.route_request(model=\"llama3.1:70b\", messages=[...])\n\n# Second request: &lt;1 min (inference only)\nresponse2 = router.route_request(model=\"llama3.1:70b\", messages=[...])\n</code></pre></p> <p>5. Monitor Performance <pre><code>response = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[...]\n)\n\n# Check routing metadata\nrouting = response.get('_routing', {})\nprint(f\"Backend: {routing.get('backend')}\")\nprint(f\"Duration: {routing.get('duration_ms')}ms\")\nprint(f\"Coordinator: {routing.get('coordinator_url')}\")\n</code></pre></p>"},{"location":"llama_cpp_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llama_cpp_guide/#issue-rpc-backends-not-found","title":"Issue: RPC Backends Not Found","text":"<p>Symptoms: <pre><code>\u26a0\ufe0f  No RPC backends found\n\ud83d\udce1 Model sharding disabled\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check RPC servers are running: <pre><code># List running RPC servers\nps aux | grep rpc-server\n\n# Should show:\n# ./build/bin/rpc-server -H 0.0.0.0 -p 50052\n</code></pre></p> </li> <li> <p>Verify network connectivity: <pre><code># Test port accessibility\nnc -zv 192.168.1.10 50052\n\n# Check firewall\nsudo ufw allow 50052\n</code></pre></p> </li> <li> <p>Enable auto-setup: <pre><code>router = HybridRouter(\n    enable_distributed=True,\n    auto_setup_rpc=True,  # Let SOLLOL build/start RPC servers\n    num_rpc_backends=3\n)\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-coordinator-startup-timeout","title":"Issue: Coordinator Startup Timeout","text":"<p>Symptoms: <pre><code>\ud83d\ude80 Starting llama.cpp coordinator...\n[waits 20+ minutes]\nTimeoutError: Coordinator failed to start\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Increase timeout: <pre><code>router = HybridRouter(\n    enable_distributed=True,\n    coordinator_timeout=1200,  # 20 minutes for 70B models\n    num_rpc_backends=3\n)\n</code></pre></p> </li> <li> <p>Check logs: <pre><code># View llama-server output\ntail -f /tmp/llama_coordinator_*.log\n</code></pre></p> </li> <li> <p>Verify GGUF exists: <pre><code>from sollol.ollama_gguf_resolver import OllamaGGUFResolver\n\nresolver = OllamaGGUFResolver()\ngguf_path = resolver.get_gguf_path(\"llama3.1:70b\")\nprint(f\"GGUF: {gguf_path}\")\n\n# Should print path like:\n# /usr/share/ollama/.ollama/models/blobs/sha256-abc123...\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-inference-timeout","title":"Issue: Inference Timeout","text":"<p>Symptoms: <pre><code>\u2705 Coordinator started successfully\n[inference request sent]\n[waits 5+ minutes]\nTimeoutError: Request timeout after 300s\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Increase request timeout: <pre><code>response = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[...],\n    timeout=600  # 10 minutes\n)\n</code></pre></p> </li> <li> <p>Check coordinator is responding: <pre><code># Test coordinator health\ncurl http://localhost:18080/health\n</code></pre></p> </li> <li> <p>Verify RPC communication: <pre><code># Check RPC backend logs\n# Look for layer assignment messages\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-coordinator-crashes-after-first-request","title":"Issue: Coordinator Crashes After First Request","text":"<p>Symptoms: <pre><code>\u2705 First inference successful\n[second request]\n\ud83d\ude80 Starting llama.cpp coordinator... (again)\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check process liveness: <pre><code># SOLLOL should detect dead processes\n# Look for: \"\u26a0\ufe0f  Coordinator process died!\"\n</code></pre></p> </li> <li> <p>Increase coordinator memory: <pre><code># Give coordinator more memory\nexport LLAMA_ARG_N_GPU_LAYERS=40\n</code></pre></p> </li> <li> <p>Check for OOM kills: <pre><code># Check system logs\ndmesg | grep -i \"out of memory\"\njournalctl -xe | grep llama\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#issue-slow-performance","title":"Issue: Slow Performance","text":"<p>Symptoms: - Inference takes 30+ seconds per token - Network appears saturated</p> <p>Solutions:</p> <ol> <li> <p>Reduce number of backends: <pre><code># Fewer backends = fewer network hops\nrouter = HybridRouter(num_rpc_backends=2)  # Instead of 4\n</code></pre></p> </li> <li> <p>Check network latency: <pre><code>ping -c 100 192.168.1.11\n# Should be &lt;10ms average\n</code></pre></p> </li> <li> <p>Use local network: <pre><code># Ensure all machines are on same LAN\n# Avoid VPN or WAN connections\n</code></pre></p> </li> </ol>"},{"location":"llama_cpp_guide/#advanced-topics","title":"Advanced Topics","text":""},{"location":"llama_cpp_guide/#custom-gguf-paths","title":"Custom GGUF Paths","text":"<p>Override automatic GGUF detection:</p> <pre><code>from sollol import HybridRouter, OllamaPool\n\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    gguf_path=\"/path/to/custom/model.gguf\"\n)\n</code></pre>"},{"location":"llama_cpp_guide/#multiple-coordinators","title":"Multiple Coordinators","text":"<p>Run different models simultaneously:</p> <pre><code># Not currently supported - coordinators are per-HybridRouter\n# Workaround: Use separate HybridRouter instances\n\nrouter_70b = HybridRouter(\n    enable_distributed=True,\n    model_filter=[\"llama3.1:70b\"]\n)\n\nrouter_405b = HybridRouter(\n    enable_distributed=True,\n    model_filter=[\"llama3.1:405b\"]\n)\n</code></pre>"},{"location":"llama_cpp_guide/#layer-distribution-strategies","title":"Layer Distribution Strategies","text":"<p>Coming soon: Custom layer distribution</p> <pre><code># Future feature\nrouter = HybridRouter(\n    enable_distributed=True,\n    layer_strategy=\"memory_aware\",  # Distribute based on VRAM\n    # or \"even\" for equal distribution\n)\n</code></pre>"},{"location":"llama_cpp_guide/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":"<p>Get detailed metrics:</p> <pre><code>stats = router.get_stats()\n\nprint(f\"Distributed requests: {stats.get('distributed_requests', 0)}\")\nprint(f\"Coordinator uptime: {stats.get('coordinator_uptime_seconds', 0)}s\")\nprint(f\"Active RPC backends: {stats.get('active_rpc_backends', 0)}\")\n</code></pre>"},{"location":"llama_cpp_guide/#see-also","title":"See Also","text":"<ul> <li>ARCHITECTURE.md - SOLLOL architecture overview</li> <li>HybridRouter API - HybridRouter documentation</li> <li>llama.cpp GitHub - llama.cpp project</li> <li>Integration Examples - More usage examples</li> </ul>"},{"location":"llama_cpp_guide/#summary","title":"Summary","text":"<p>SOLLOL's llama.cpp integration makes model sharding accessible:</p> <p>\u2705 Easy Setup - Auto-discovery and auto-setup \u2705 Intelligent Routing - Automatic backend selection \u2705 GGUF Extraction - No manual file management \u2705 Hybrid Operation - Small models stay fast, large models become possible \u2705 Production Ready - Coordinator reuse, health checking, failover</p> <p>Quick Start: <pre><code>from sollol.sync_wrapper import HybridRouter, OllamaPool\n\nrouter = HybridRouter(\n    ollama_pool=OllamaPool.auto_configure(),\n    enable_distributed=True,\n    auto_setup_rpc=True,\n    num_rpc_backends=3\n)\n\n# Just use it - SOLLOL handles the rest\nresponse = router.route_request(\n    model=\"llama3.1:70b\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre></p> <p>That's it! \ud83d\ude80</p>"},{"location":"enterprise/roadmap/","title":"Enterprise Roadmap","text":""},{"location":"enterprise/roadmap/#future-enterprise-opportunities","title":"Future Enterprise Opportunities","text":"<p>The free version includes everything you need for production deployments. These advanced features are available through custom development partnerships and sponsorship.</p>"},{"location":"enterprise/roadmap/#available-enterprise-features","title":"Available Enterprise Features","text":""},{"location":"enterprise/roadmap/#ray-train-integration","title":"\ud83d\udd27 Ray Train Integration","text":"<p>Distributed model fine-tuning across GPU clusters</p> <p>Train custom LLMs on your infrastructure with distributed training:</p> <ul> <li>Multi-node GPU orchestration</li> <li>Automatic checkpointing and recovery</li> <li>Distributed data loading</li> <li>Training metrics and visualization</li> <li>Integration with your existing models</li> </ul> <p>Use Case: Organizations training custom models on proprietary data</p>"},{"location":"enterprise/roadmap/#multi-region-orchestration","title":"\ud83c\udf10 Multi-Region Orchestration","text":"<p>Global load balancing with geo-aware routing</p> <p>Deploy SOLLOL across multiple regions with intelligent geo-routing:</p> <ul> <li>Latency-based routing (&lt;100ms worldwide)</li> <li>Regional failover and disaster recovery</li> <li>Data sovereignty compliance</li> <li>Cross-region traffic management</li> <li>Global health monitoring</li> </ul> <p>Use Case: Worldwide SaaS platforms requiring low-latency AI inference</p>"},{"location":"enterprise/roadmap/#advanced-analytics-suite","title":"\ud83d\udcca Advanced Analytics Suite","text":"<p>ML-powered capacity planning and cost optimization</p> <p>Predictive analytics for optimal resource utilization:</p> <ul> <li>ML-based demand forecasting</li> <li>Automated capacity recommendations</li> <li>Cost optimization algorithms</li> <li>Budget alerts and reporting</li> <li>Usage trend analysis</li> </ul> <p>Use Case: Enterprises optimizing cloud spend and resource allocation</p>"},{"location":"enterprise/roadmap/#enterprise-sso-integration","title":"\ud83d\udd10 Enterprise SSO Integration","text":"<p>SAML, OAuth2, LDAP, Active Directory</p> <p>Enterprise identity management integration:</p> <ul> <li>Single sign-on (SAML 2.0)</li> <li>OAuth2/OIDC providers</li> <li>LDAP/Active Directory sync</li> <li>Role mapping and provisioning</li> <li>Audit logging</li> </ul> <p>Use Case: Large organizations with existing identity infrastructure</p>"},{"location":"enterprise/roadmap/#custom-routing-engines","title":"\ud83c\udfaf Custom Routing Engines","text":"<p>Bespoke algorithms for specialized workloads</p> <p>Industry-specific routing optimizations:</p> <ul> <li>Custom scoring algorithms</li> <li>Domain-specific task detection</li> <li>Specialized performance metrics</li> <li>Workflow-aware routing</li> <li>Integration with internal systems</li> </ul> <p>Use Case: Companies with unique workload patterns or compliance requirements</p>"},{"location":"enterprise/roadmap/#sla-guarantees","title":"\ud83d\udee1\ufe0f SLA Guarantees","text":"<p>99.9%+ uptime with priority support</p> <p>Enterprise-grade reliability and support:</p> <ul> <li>99.9% uptime SLA</li> <li>4-hour incident response</li> <li>Dedicated support engineer</li> <li>Quarterly architecture reviews</li> <li>Custom runbooks and documentation</li> </ul> <p>Use Case: Mission-critical production systems requiring guaranteed uptime</p>"},{"location":"enterprise/roadmap/#dedicated-support","title":"\ud83d\udcde Dedicated Support","text":"<p>Hands-on partnership and architecture reviews</p> <p>Direct access to SOLLOL experts:</p> <ul> <li>Private Slack channel</li> <li>Weekly video calls</li> <li>Architecture design sessions</li> <li>Performance optimization reviews</li> <li>On-call engineering support</li> </ul> <p>Use Case: Teams needing expert guidance for complex deployments</p>"},{"location":"enterprise/roadmap/#custom-development","title":"\ud83c\udfd7\ufe0f Custom Development","text":"<p>Tailored features for your infrastructure</p> <p>Bespoke development services:</p> <ul> <li>Custom integrations (monitoring, logging, etc.)</li> <li>Proprietary protocol support</li> <li>Internal tool integration</li> <li>Feature development</li> <li>Migration assistance</li> </ul> <p>Use Case: Organizations with unique technical requirements</p>"},{"location":"enterprise/roadmap/#why-sponsorship","title":"Why Sponsorship?","text":"<p>Each enterprise feature involves:</p> <ul> <li>Months of development time - Complex features requiring significant engineering effort</li> <li>Enterprise integrations - Testing with SAML providers, LDAP servers, cloud platforms</li> <li>Ongoing maintenance - Security patches, compatibility updates, bug fixes</li> <li>Multi-environment testing - Validation across different infrastructure setups</li> <li>Comprehensive documentation - Architecture guides, deployment runbooks, training materials</li> </ul>"},{"location":"enterprise/roadmap/#get-started","title":"Get Started","text":""},{"location":"enterprise/roadmap/#engagement-process","title":"Engagement Process","text":"<ol> <li>Discovery Call - Discuss your requirements and use case</li> <li>Proposal - Detailed scope, timeline, and pricing</li> <li>Development - Fixed-price or retainer-based engagement</li> <li>Delivery - Feature implementation with documentation</li> <li>Support - Ongoing maintenance and updates</li> </ol>"},{"location":"enterprise/roadmap/#contact","title":"Contact","text":"<ul> <li>GitHub Sponsors: Sponsor SOLLOL Development</li> <li>Discussions: Start a conversation</li> <li>Email: Open a discussion for direct contact</li> </ul>"},{"location":"enterprise/roadmap/#pricing-models","title":"Pricing Models","text":"<p>Fixed-Price Projects - Defined scope and deliverables - Clear timeline and milestones - Typical range: $50k - $200k per feature</p> <p>Retainer Agreements - Monthly commitment for ongoing development - Flexible scope and priorities - Includes support and maintenance - Typical range: $20k - $50k/month</p> <p>Custom Packages - Combination of features - Volume discounts available - Long-term partnerships</p>"},{"location":"enterprise/roadmap/#existing-clients","title":"Existing Clients","text":"<p>Coming soon - enterprise client case studies and testimonials</p>"},{"location":"enterprise/roadmap/#faq","title":"FAQ","text":"<p>Q: Can I contribute enterprise features as open source?</p> <p>A: Absolutely! If you develop a feature and want to contribute it back, we welcome PRs. Enterprise features are those requiring sponsored development effort.</p> <p>Q: How long does custom development take?</p> <p>A: Typically 3-6 months per major feature, depending on complexity and integration requirements.</p> <p>Q: Do you offer proof-of-concept work?</p> <p>A: Yes, we can start with a small PoC (2-4 weeks) to validate feasibility before full development.</p> <p>Q: What happens if my needs change?</p> <p>A: Retainer agreements offer flexibility to adjust priorities. Fixed-price projects have change order processes.</p> <p>Ready to discuss enterprise features? Start a conversation \u2192</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get SOLLOL running in 5 minutes with Docker Compose.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed</li> <li>8GB+ RAM recommended</li> <li>(Optional) GPU for optimal performance</li> </ul>"},{"location":"getting-started/quick-start/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/BenevolentJoker-JohnL/SOLLOL.git\ncd SOLLOL\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-start-the-stack","title":"Step 2: Start the Stack","text":"<pre><code># Start SOLLOL + 3 Ollama nodes + Prometheus + Grafana\ndocker-compose up -d\n\n# Check status\ndocker-compose ps\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-pull-a-model","title":"Step 3: Pull a Model","text":"<pre><code># Pull llama3.2 on all nodes\ndocker exec sollol-ollama-node-1-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-2-1 ollama pull llama3.2\ndocker exec sollol-ollama-node-3-1 ollama pull llama3.2\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-test-the-setup","title":"Step 4: Test the Setup","text":"<pre><code># Send a test request to SOLLOL (drop-in replacement on port 11434)\ncurl -X POST http://localhost:11434/api/chat \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llama3.2\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n\n# Or use the standard Ollama API (SOLLOL is transparent!)\nexport OLLAMA_HOST=localhost:11434\nollama run llama3.2 \"Hello!\"\n</code></pre>"},{"location":"getting-started/quick-start/#step-5-view-the-dashboard","title":"Step 5: View the Dashboard","text":"<p>Open your browser:</p> <ul> <li>SOLLOL Dashboard: http://localhost:11434/dashboard.html</li> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Prometheus: http://localhost:9091</li> </ul>"},{"location":"getting-started/quick-start/#using-the-python-sdk","title":"Using the Python SDK","text":"<pre><code>from sollol import connect\n\n# Connect to SOLLOL (drop-in replacement - same port as Ollama!)\nsollol = connect(\"http://localhost:11434\")\n\n# Chat with intelligent routing\nresponse = sollol.chat(\n    \"Explain quantum computing\",\n    priority=8  # High priority = faster nodes\n)\n\nprint(response['message']['content'])\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Customize SOLLOL settings</li> <li>Architecture - Understand how it works</li> <li>Deployment - Deploy to production</li> <li>Benchmarks - Run performance tests</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#ollama-nodes-not-responding","title":"Ollama nodes not responding","text":"<pre><code># Check logs\ndocker-compose logs sollol\ndocker-compose logs ollama-node-1\n\n# Restart a node\ndocker-compose restart ollama-node-1\n</code></pre>"},{"location":"getting-started/quick-start/#port-conflicts","title":"Port conflicts","text":"<p>If port 11434 is already in use (e.g., you have Ollama running), stop it first:</p> <pre><code># Stop standalone Ollama (if running)\npkill ollama\n\n# Or change SOLLOL's port in docker-compose.yml\nports:\n  - \"11435:11434\"  # Change external port\n</code></pre>"},{"location":"getting-started/quick-start/#gpu-not-detected","title":"GPU not detected","text":"<p>Ensure NVIDIA Container Toolkit is installed:</p> <pre><code># Install NVIDIA Container Toolkit\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n</code></pre> <p>Then uncomment GPU sections in <code>docker-compose.yml</code>.</p>"},{"location":"getting-started/quick-start/#support","title":"Support","text":"<p>Need help? Open an issue on GitHub.</p>"}]}